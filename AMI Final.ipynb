{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.3.2 in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==4.3.2) (1.18.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==4.3.2) (2021.11.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==4.3.2) (20.4)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==4.3.2) (0.0.46)\n",
      "Requirement already satisfied: requests in c:\\users\\komol\\appdata\\roaming\\python\\python37\\site-packages (from transformers==4.3.2) (2.21.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==4.3.2) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==4.3.2) (4.8.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==4.3.2) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from transformers==4.3.2) (4.57.0)\n",
      "Requirement already satisfied: six in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging->transformers==4.3.2) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from packaging->transformers==4.3.2) (2.4.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sacremoses->transformers==4.3.2) (0.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sacremoses->transformers==4.3.2) (7.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->transformers==4.3.2) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\komol\\appdata\\roaming\\python\\python37\\site-packages (from requests->transformers==4.3.2) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests->transformers==4.3.2) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\komol\\appdata\\roaming\\python\\python37\\site-packages (from requests->transformers==4.3.2) (1.24.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.3.2) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.3.2) (3.10.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.3.2\n",
    "import pandas as pd\n",
    "import torch\n",
    "import io\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "from transformers import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install sentencepiece\n",
    "\n",
    "##Set random values\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ganbert' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------\n",
    "#  Transformer parameters\n",
    "#--------------------------------\n",
    "max_seq_length = 64\n",
    "batch_size = 4\n",
    "\n",
    "#--------------------------------\n",
    "#  GAN-BERT specific parameters\n",
    "#--------------------------------\n",
    "# number of hidden layers in the generator, \n",
    "# each of the size of the output space\n",
    "num_hidden_layers_g = 1; \n",
    "# number of hidden layers in the discriminator, \n",
    "# each of the size of the input space\n",
    "num_hidden_layers_d = 1; \n",
    "# size of the generator's input noisy vectors\n",
    "noise_size = 100\n",
    "# dropout to be applied to discriminator's input vectors\n",
    "out_dropout_rate = 0.2\n",
    "\n",
    "# Replicate labeled data to balance poorly represented datasets, \n",
    "# e.g., less than 1% of labeled material\n",
    "apply_balance = True\n",
    "\n",
    "#--------------------------------\n",
    "#  Optimization parameters\n",
    "#--------------------------------\n",
    "learning_rate_discriminator = 5e-5\n",
    "learning_rate_generator = 5e-5\n",
    "epsilon = 1e-8\n",
    "num_train_epochs = 1\n",
    "multi_gpu = True\n",
    "# Scheduler\n",
    "apply_scheduler = False\n",
    "warmup_proportion = 0.1\n",
    "# Print\n",
    "print_each_n_step = 10\n",
    "\n",
    "#--------------------------------\n",
    "#  Adopted Tranformer model\n",
    "#--------------------------------\n",
    "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
    "# (or add) transformer models compatible with GAN\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#model_name = \"roberta-base\"\n",
    "#model_name = \"albert-base-v2\"\n",
    "#model_name = \"xlm-roberta-base\"\n",
    "#model_name = \"amazon/bort\"\n",
    "\n",
    "#--------------------------------\n",
    "#  Retrieve the TREC QC Dataset\n",
    "#--------------------------------\n",
    "! git clone https://github.com/crux82/ganbert\n",
    "\n",
    "#  NOTE: in this setting 50 classes are involved\n",
    "#labeled_file = \"./ganbert/data/labeled.tsv\"\n",
    "#unlabeled_file = \"./ganbert/data/unlabeled.tsv\"\n",
    "#test_filename = \"./ganbert/data/test.tsv\"\n",
    "\n",
    "# label_list = [\"UNK_UNK\",\"ABBR_abb\", \"ABBR_exp\", \"DESC_def\", \"DESC_desc\", \n",
    "#               \"DESC_manner\", \"DESC_reason\", \"ENTY_animal\", \"ENTY_body\", \n",
    "#               \"ENTY_color\", \"ENTY_cremat\", \"ENTY_currency\", \"ENTY_dismed\", \n",
    "#               \"ENTY_event\", \"ENTY_food\", \"ENTY_instru\", \"ENTY_lang\", \n",
    "#               \"ENTY_letter\", \"ENTY_other\", \"ENTY_plant\", \"ENTY_product\", \n",
    "#               \"ENTY_religion\", \"ENTY_sport\", \"ENTY_substance\", \"ENTY_symbol\", \n",
    "#               \"ENTY_techmeth\", \"ENTY_termeq\", \"ENTY_veh\", \"ENTY_word\", \"HUM_desc\", \n",
    "#               \"HUM_gr\", \"HUM_ind\", \"HUM_title\", \"LOC_city\", \"LOC_country\", \n",
    "#               \"LOC_mount\", \"LOC_other\", \"LOC_state\", \"NUM_code\", \"NUM_count\", \n",
    "#               \"NUM_date\", \"NUM_dist\", \"NUM_money\", \"NUM_ord\", \"NUM_other\", \n",
    "#               \"NUM_perc\", \"NUM_period\", \"NUM_speed\", \"NUM_temp\", \"NUM_volsize\", \n",
    "#               \"NUM_weight\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.getcwd()\n",
    "\n",
    "#!cp gdrive/MyDrive/Colab\\ Notebooks/Corona_NLP_test.csv ganbert/data/\n",
    "#!cp gdrive/MyDrive/Colab\\ Notebooks/Corona_NLP_train.csv ganbert/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(r'C:\\Users\\Komol\\one drive\\OneDrive - Queensland University of Technology\\H-Migrated\\Jupyter_Code_ipynb\\Research Officer SEF\\train.csv', encoding='latin-1')\n",
    "\n",
    "df_test = pd.read_csv(r'C:\\Users\\Komol\\one drive\\OneDrive - Queensland University of Technology\\H-Migrated\\Jupyter_Code_ipynb\\Research Officer SEF\\test.csv', encoding='latin-1')\n",
    "df_unlabeled= pd.read_csv(r'C:\\Users\\Komol\\one drive\\OneDrive - Queensland University of Technology\\H-Migrated\\Jupyter_Code_ipynb\\Research Officer SEF\\Unlabelled\\50_200_selected_combined.csv', encoding='latin-1')\n",
    "df_train.head()\n",
    "print(df_train.label.unique())\n",
    "#df_train = df_train.sample(frac=0.1)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_list = ['Neutral', 'Positive', 'Extremely Negative', 'Negative', 'Extremely Positive','UNK']\n",
    "label_list = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabeled=df_unlabeled.drop(['id'],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change each of the sentiment for unlabeled data to Unknown\n",
    "for i in df_unlabeled.index:\n",
    "    df_unlabeled.at[i, \"label\"] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>i love how there always seems to be an outage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>xrtu: it's time to decide once and for all whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>xrtu: it's never too late to dream.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>maga has ruined all of us â ï¸</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>xrtu: lord guide and protect the entire caribb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38266</th>\n",
       "      <td>2</td>\n",
       "      <td>xrtu: please stop what you're doing right now ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38267</th>\n",
       "      <td>2</td>\n",
       "      <td>baby you so fine to me, but i ain't tryna love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38268</th>\n",
       "      <td>2</td>\n",
       "      <td>bitch i'm still alive?? xurl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38269</th>\n",
       "      <td>2</td>\n",
       "      <td>xatp no problem. will do.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38270</th>\n",
       "      <td>2</td>\n",
       "      <td>this week is just weird. i keep thinking it's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38271 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "0          2  i love how there always seems to be an outage ...\n",
       "1          2  xrtu: it's time to decide once and for all whi...\n",
       "2          2                xrtu: it's never too late to dream.\n",
       "3          2                   maga has ruined all of us â ï¸\n",
       "4          2  xrtu: lord guide and protect the entire caribb...\n",
       "...      ...                                                ...\n",
       "38266      2  xrtu: please stop what you're doing right now ...\n",
       "38267      2  baby you so fine to me, but i ain't tryna love...\n",
       "38268      2                       bitch i'm still alive?? xurl\n",
       "38269      2                          xatp no problem. will do.\n",
       "38270      2  this week is just weird. i keep thinking it's ...\n",
       "\n",
       "[38271 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 2) (1000, 2) (38271, 2)\n"
     ]
    }
   ],
   "source": [
    "#df_train_for_ganbert = df_train.sample(frac=0.01) # use 1% of the labeled data for training\n",
    "#df_unlabeled = df_train.drop(df_train_for_ganbert.index)\n",
    "\n",
    "print(df_train.shape,df_test.shape, df_unlabeled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to format the dataset to be used in the dataloader\n",
    "def get_examples(df):\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    # iterate through each row\n",
    "    for index, row in df.iterrows():\n",
    "        examples.append((row['text'], row['label']))\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_examples = get_examples(df_train)\n",
    "unlabeled_examples = get_examples(df_unlabeled)\n",
    "test_examples = get_examples(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer, 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
    "  '''\n",
    "  Generate a Dataloader given the input examples, eventually masked if they are \n",
    "  to be considered NOT labeled.\n",
    "  '''\n",
    "  examples = []\n",
    "\n",
    "  # Count the percentage of labeled examples  \n",
    "  num_labeled_examples = 0\n",
    "  for label_mask in label_masks:\n",
    "    if label_mask: \n",
    "      num_labeled_examples += 1\n",
    "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
    "\n",
    "  # if required it applies the balance\n",
    "  for index, ex in enumerate(input_examples): \n",
    "    if label_mask_rate == 1 or not balance_label_examples:\n",
    "      examples.append((ex, label_masks[index]))\n",
    "    else:\n",
    "      # IT SIMULATE A LABELED EXAMPLE\n",
    "      if label_masks[index]:\n",
    "        balance = int(1/label_mask_rate)\n",
    "        balance = int(math.log(balance,2))\n",
    "        if balance < 1:\n",
    "          balance = 1\n",
    "        for b in range(0, int(balance)):\n",
    "          examples.append((ex, label_masks[index]))\n",
    "      else:\n",
    "        examples.append((ex, label_masks[index]))\n",
    "  \n",
    "  #-----------------------------------------------\n",
    "  # Generate input examples to the Transformer\n",
    "  #-----------------------------------------------\n",
    "  input_ids = []\n",
    "  input_mask_array = []\n",
    "  label_mask_array = []\n",
    "  label_id_array = []\n",
    "\n",
    "  # Tokenization \n",
    "  for (text, label_mask) in examples:\n",
    "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "    label_id_array.append(label_map[text[1]])\n",
    "    label_mask_array.append(label_mask)\n",
    "  \n",
    "  # Attention to token (to ignore padded input wordpieces)\n",
    "  for sent in input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
    "    input_mask_array.append(att_mask)\n",
    "  # Convertion to Tensor\n",
    "  input_ids = torch.tensor(input_ids) \n",
    "  input_mask_array = torch.tensor(input_mask_array)\n",
    "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
    "  label_mask_array = torch.tensor(label_mask_array)\n",
    "\n",
    "  # Building the TensorDataset\n",
    "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
    "\n",
    "  if do_shuffle:\n",
    "    sampler = RandomSampler\n",
    "  else:\n",
    "    sampler = SequentialSampler\n",
    "\n",
    "  # Building the DataLoader\n",
    "  return DataLoader(\n",
    "              dataset,  # The training samples.\n",
    "              sampler = sampler(dataset), \n",
    "              batch_size = batch_size, # Trains with this batch size.\n",
    "              drop_last=True) \n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "for (i, label) in enumerate(label_list):\n",
    "  label_map[label] = i\n",
    "#------------------------------\n",
    "#   Load the train dataset\n",
    "#------------------------------\n",
    "train_examples = labeled_examples\n",
    "#The labeled (train) dataset is assigned with a mask set to True\n",
    "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
    "#If unlabel examples are available\n",
    "if unlabeled_examples:\n",
    "  train_examples = train_examples + unlabeled_examples\n",
    "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
    "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
    "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
    "\n",
    "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
    "\n",
    "#------------------------------\n",
    "#   Load the test dataset\n",
    "#------------------------------\n",
    "#The labeled (test) dataset is assigned with a mask set to True\n",
    "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
    "\n",
    "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "#   The Generator as in \n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
    "        super(Generator, self).__init__()\n",
    "        layers = []\n",
    "        hidden_sizes = [noise_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        output_rep = self.layers(noise)\n",
    "        return output_rep\n",
    "\n",
    "#------------------------------\n",
    "#   The Discriminator\n",
    "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
    "#   https://github.com/crux82/ganbert\n",
    "#------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
    "        layers = []\n",
    "        hidden_sizes = [input_size] + hidden_sizes\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
    "\n",
    "        self.layers = nn.Sequential(*layers) #per il flatten\n",
    "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_rep):\n",
    "        input_rep = self.input_dropout(input_rep)\n",
    "        last_rep = self.layers(input_rep)\n",
    "        logits = self.logit(last_rep)\n",
    "        probs = self.softmax(logits)\n",
    "        return last_rep, logits, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.3.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The config file is required to get the dimension of the vector produced by \n",
    "# the underlying transformer\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "hidden_size = int(config.hidden_size)\n",
    "# Define the number and width of hidden layers\n",
    "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
    "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
    "\n",
    "#-------------------------------------------------\n",
    "#   Instantiate the Generator and Discriminator\n",
    "#-------------------------------------------------\n",
    "generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
    "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
    "\n",
    "# Put everything in the GPU if available\n",
    "if torch.cuda.is_available():    \n",
    "  generator.cuda()\n",
    "  discriminator.cuda()\n",
    "  transformer.cuda()\n",
    "  if multi_gpu:\n",
    "    transformer = torch.nn.DataParallel(transformer)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch    10  of  12,567.    Elapsed: 0:00:03.\n",
      "  Batch    20  of  12,567.    Elapsed: 0:00:06.\n",
      "  Batch    30  of  12,567.    Elapsed: 0:00:09.\n",
      "  Batch    40  of  12,567.    Elapsed: 0:00:12.\n",
      "  Batch    50  of  12,567.    Elapsed: 0:00:15.\n",
      "  Batch    60  of  12,567.    Elapsed: 0:00:18.\n",
      "  Batch    70  of  12,567.    Elapsed: 0:00:21.\n",
      "  Batch    80  of  12,567.    Elapsed: 0:00:24.\n",
      "  Batch    90  of  12,567.    Elapsed: 0:00:27.\n",
      "  Batch   100  of  12,567.    Elapsed: 0:00:30.\n",
      "  Batch   110  of  12,567.    Elapsed: 0:00:33.\n",
      "  Batch   120  of  12,567.    Elapsed: 0:00:36.\n",
      "  Batch   130  of  12,567.    Elapsed: 0:00:39.\n",
      "  Batch   140  of  12,567.    Elapsed: 0:00:42.\n",
      "  Batch   150  of  12,567.    Elapsed: 0:00:45.\n",
      "  Batch   160  of  12,567.    Elapsed: 0:00:48.\n",
      "  Batch   170  of  12,567.    Elapsed: 0:00:51.\n",
      "  Batch   180  of  12,567.    Elapsed: 0:00:54.\n",
      "  Batch   190  of  12,567.    Elapsed: 0:00:57.\n",
      "  Batch   200  of  12,567.    Elapsed: 0:01:00.\n",
      "  Batch   210  of  12,567.    Elapsed: 0:01:03.\n",
      "  Batch   220  of  12,567.    Elapsed: 0:01:06.\n",
      "  Batch   230  of  12,567.    Elapsed: 0:01:09.\n",
      "  Batch   240  of  12,567.    Elapsed: 0:01:12.\n",
      "  Batch   250  of  12,567.    Elapsed: 0:01:15.\n",
      "  Batch   260  of  12,567.    Elapsed: 0:01:18.\n",
      "  Batch   270  of  12,567.    Elapsed: 0:01:21.\n",
      "  Batch   280  of  12,567.    Elapsed: 0:01:24.\n",
      "  Batch   290  of  12,567.    Elapsed: 0:01:26.\n",
      "  Batch   300  of  12,567.    Elapsed: 0:01:29.\n",
      "  Batch   310  of  12,567.    Elapsed: 0:01:32.\n",
      "  Batch   320  of  12,567.    Elapsed: 0:01:35.\n",
      "  Batch   330  of  12,567.    Elapsed: 0:01:38.\n",
      "  Batch   340  of  12,567.    Elapsed: 0:01:41.\n",
      "  Batch   350  of  12,567.    Elapsed: 0:01:44.\n",
      "  Batch   360  of  12,567.    Elapsed: 0:01:47.\n",
      "  Batch   370  of  12,567.    Elapsed: 0:01:50.\n",
      "  Batch   380  of  12,567.    Elapsed: 0:01:53.\n",
      "  Batch   390  of  12,567.    Elapsed: 0:01:56.\n",
      "  Batch   400  of  12,567.    Elapsed: 0:01:59.\n",
      "  Batch   410  of  12,567.    Elapsed: 0:02:02.\n",
      "  Batch   420  of  12,567.    Elapsed: 0:02:05.\n",
      "  Batch   430  of  12,567.    Elapsed: 0:02:08.\n",
      "  Batch   440  of  12,567.    Elapsed: 0:02:11.\n",
      "  Batch   450  of  12,567.    Elapsed: 0:02:14.\n",
      "  Batch   460  of  12,567.    Elapsed: 0:02:17.\n",
      "  Batch   470  of  12,567.    Elapsed: 0:02:20.\n",
      "  Batch   480  of  12,567.    Elapsed: 0:02:23.\n",
      "  Batch   490  of  12,567.    Elapsed: 0:02:26.\n",
      "  Batch   500  of  12,567.    Elapsed: 0:02:29.\n",
      "  Batch   510  of  12,567.    Elapsed: 0:02:32.\n",
      "  Batch   520  of  12,567.    Elapsed: 0:02:35.\n",
      "  Batch   530  of  12,567.    Elapsed: 0:02:37.\n",
      "  Batch   540  of  12,567.    Elapsed: 0:02:40.\n",
      "  Batch   550  of  12,567.    Elapsed: 0:02:43.\n",
      "  Batch   560  of  12,567.    Elapsed: 0:02:46.\n",
      "  Batch   570  of  12,567.    Elapsed: 0:02:49.\n",
      "  Batch   580  of  12,567.    Elapsed: 0:02:52.\n",
      "  Batch   590  of  12,567.    Elapsed: 0:02:55.\n",
      "  Batch   600  of  12,567.    Elapsed: 0:02:58.\n",
      "  Batch   610  of  12,567.    Elapsed: 0:03:01.\n",
      "  Batch   620  of  12,567.    Elapsed: 0:03:04.\n",
      "  Batch   630  of  12,567.    Elapsed: 0:03:08.\n",
      "  Batch   640  of  12,567.    Elapsed: 0:03:11.\n",
      "  Batch   650  of  12,567.    Elapsed: 0:03:14.\n",
      "  Batch   660  of  12,567.    Elapsed: 0:03:17.\n",
      "  Batch   670  of  12,567.    Elapsed: 0:03:19.\n",
      "  Batch   680  of  12,567.    Elapsed: 0:03:22.\n",
      "  Batch   690  of  12,567.    Elapsed: 0:03:25.\n",
      "  Batch   700  of  12,567.    Elapsed: 0:03:28.\n",
      "  Batch   710  of  12,567.    Elapsed: 0:03:31.\n",
      "  Batch   720  of  12,567.    Elapsed: 0:03:34.\n",
      "  Batch   730  of  12,567.    Elapsed: 0:03:37.\n",
      "  Batch   740  of  12,567.    Elapsed: 0:03:40.\n",
      "  Batch   750  of  12,567.    Elapsed: 0:03:43.\n",
      "  Batch   760  of  12,567.    Elapsed: 0:03:46.\n",
      "  Batch   770  of  12,567.    Elapsed: 0:03:49.\n",
      "  Batch   780  of  12,567.    Elapsed: 0:03:52.\n",
      "  Batch   790  of  12,567.    Elapsed: 0:03:55.\n",
      "  Batch   800  of  12,567.    Elapsed: 0:03:58.\n",
      "  Batch   810  of  12,567.    Elapsed: 0:04:01.\n",
      "  Batch   820  of  12,567.    Elapsed: 0:04:04.\n",
      "  Batch   830  of  12,567.    Elapsed: 0:04:07.\n",
      "  Batch   840  of  12,567.    Elapsed: 0:04:10.\n",
      "  Batch   850  of  12,567.    Elapsed: 0:04:13.\n",
      "  Batch   860  of  12,567.    Elapsed: 0:04:16.\n",
      "  Batch   870  of  12,567.    Elapsed: 0:04:19.\n",
      "  Batch   880  of  12,567.    Elapsed: 0:04:22.\n",
      "  Batch   890  of  12,567.    Elapsed: 0:04:25.\n",
      "  Batch   900  of  12,567.    Elapsed: 0:04:28.\n",
      "  Batch   910  of  12,567.    Elapsed: 0:04:31.\n",
      "  Batch   920  of  12,567.    Elapsed: 0:04:34.\n",
      "  Batch   930  of  12,567.    Elapsed: 0:04:37.\n",
      "  Batch   940  of  12,567.    Elapsed: 0:04:40.\n",
      "  Batch   950  of  12,567.    Elapsed: 0:04:43.\n",
      "  Batch   960  of  12,567.    Elapsed: 0:04:46.\n",
      "  Batch   970  of  12,567.    Elapsed: 0:04:49.\n",
      "  Batch   980  of  12,567.    Elapsed: 0:04:52.\n",
      "  Batch   990  of  12,567.    Elapsed: 0:04:55.\n",
      "  Batch 1,000  of  12,567.    Elapsed: 0:04:58.\n",
      "  Batch 1,010  of  12,567.    Elapsed: 0:05:01.\n",
      "  Batch 1,020  of  12,567.    Elapsed: 0:05:04.\n",
      "  Batch 1,030  of  12,567.    Elapsed: 0:05:07.\n",
      "  Batch 1,040  of  12,567.    Elapsed: 0:05:10.\n",
      "  Batch 1,050  of  12,567.    Elapsed: 0:05:13.\n",
      "  Batch 1,060  of  12,567.    Elapsed: 0:05:16.\n",
      "  Batch 1,070  of  12,567.    Elapsed: 0:05:19.\n",
      "  Batch 1,080  of  12,567.    Elapsed: 0:05:22.\n",
      "  Batch 1,090  of  12,567.    Elapsed: 0:05:25.\n",
      "  Batch 1,100  of  12,567.    Elapsed: 0:05:28.\n",
      "  Batch 1,110  of  12,567.    Elapsed: 0:05:31.\n",
      "  Batch 1,120  of  12,567.    Elapsed: 0:05:34.\n",
      "  Batch 1,130  of  12,567.    Elapsed: 0:05:37.\n",
      "  Batch 1,140  of  12,567.    Elapsed: 0:05:40.\n",
      "  Batch 1,150  of  12,567.    Elapsed: 0:05:43.\n",
      "  Batch 1,160  of  12,567.    Elapsed: 0:05:46.\n",
      "  Batch 1,170  of  12,567.    Elapsed: 0:05:49.\n",
      "  Batch 1,180  of  12,567.    Elapsed: 0:05:52.\n",
      "  Batch 1,190  of  12,567.    Elapsed: 0:05:55.\n",
      "  Batch 1,200  of  12,567.    Elapsed: 0:05:58.\n",
      "  Batch 1,210  of  12,567.    Elapsed: 0:06:01.\n",
      "  Batch 1,220  of  12,567.    Elapsed: 0:06:04.\n",
      "  Batch 1,230  of  12,567.    Elapsed: 0:06:07.\n",
      "  Batch 1,240  of  12,567.    Elapsed: 0:06:10.\n",
      "  Batch 1,250  of  12,567.    Elapsed: 0:06:13.\n",
      "  Batch 1,260  of  12,567.    Elapsed: 0:06:15.\n",
      "  Batch 1,270  of  12,567.    Elapsed: 0:06:18.\n",
      "  Batch 1,280  of  12,567.    Elapsed: 0:06:21.\n",
      "  Batch 1,290  of  12,567.    Elapsed: 0:06:24.\n",
      "  Batch 1,300  of  12,567.    Elapsed: 0:06:27.\n",
      "  Batch 1,310  of  12,567.    Elapsed: 0:06:30.\n",
      "  Batch 1,320  of  12,567.    Elapsed: 0:06:33.\n",
      "  Batch 1,330  of  12,567.    Elapsed: 0:06:36.\n",
      "  Batch 1,340  of  12,567.    Elapsed: 0:06:39.\n",
      "  Batch 1,350  of  12,567.    Elapsed: 0:06:42.\n",
      "  Batch 1,360  of  12,567.    Elapsed: 0:06:45.\n",
      "  Batch 1,370  of  12,567.    Elapsed: 0:06:48.\n",
      "  Batch 1,380  of  12,567.    Elapsed: 0:06:51.\n",
      "  Batch 1,390  of  12,567.    Elapsed: 0:06:54.\n",
      "  Batch 1,400  of  12,567.    Elapsed: 0:06:57.\n",
      "  Batch 1,410  of  12,567.    Elapsed: 0:07:00.\n",
      "  Batch 1,420  of  12,567.    Elapsed: 0:07:03.\n",
      "  Batch 1,430  of  12,567.    Elapsed: 0:07:06.\n",
      "  Batch 1,440  of  12,567.    Elapsed: 0:07:09.\n",
      "  Batch 1,450  of  12,567.    Elapsed: 0:07:12.\n",
      "  Batch 1,460  of  12,567.    Elapsed: 0:07:15.\n",
      "  Batch 1,470  of  12,567.    Elapsed: 0:07:18.\n",
      "  Batch 1,480  of  12,567.    Elapsed: 0:07:21.\n",
      "  Batch 1,490  of  12,567.    Elapsed: 0:07:24.\n",
      "  Batch 1,500  of  12,567.    Elapsed: 0:07:27.\n",
      "  Batch 1,510  of  12,567.    Elapsed: 0:07:30.\n",
      "  Batch 1,520  of  12,567.    Elapsed: 0:07:33.\n",
      "  Batch 1,530  of  12,567.    Elapsed: 0:07:36.\n",
      "  Batch 1,540  of  12,567.    Elapsed: 0:07:39.\n",
      "  Batch 1,550  of  12,567.    Elapsed: 0:07:42.\n",
      "  Batch 1,560  of  12,567.    Elapsed: 0:07:45.\n",
      "  Batch 1,570  of  12,567.    Elapsed: 0:07:48.\n",
      "  Batch 1,580  of  12,567.    Elapsed: 0:07:50.\n",
      "  Batch 1,590  of  12,567.    Elapsed: 0:07:53.\n",
      "  Batch 1,600  of  12,567.    Elapsed: 0:07:56.\n",
      "  Batch 1,610  of  12,567.    Elapsed: 0:07:59.\n",
      "  Batch 1,620  of  12,567.    Elapsed: 0:08:02.\n",
      "  Batch 1,630  of  12,567.    Elapsed: 0:08:05.\n",
      "  Batch 1,640  of  12,567.    Elapsed: 0:08:08.\n",
      "  Batch 1,650  of  12,567.    Elapsed: 0:08:11.\n",
      "  Batch 1,660  of  12,567.    Elapsed: 0:08:14.\n",
      "  Batch 1,670  of  12,567.    Elapsed: 0:08:17.\n",
      "  Batch 1,680  of  12,567.    Elapsed: 0:08:20.\n",
      "  Batch 1,690  of  12,567.    Elapsed: 0:08:23.\n",
      "  Batch 1,700  of  12,567.    Elapsed: 0:08:26.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1,710  of  12,567.    Elapsed: 0:08:29.\n",
      "  Batch 1,720  of  12,567.    Elapsed: 0:08:32.\n",
      "  Batch 1,730  of  12,567.    Elapsed: 0:08:35.\n",
      "  Batch 1,740  of  12,567.    Elapsed: 0:08:38.\n",
      "  Batch 1,750  of  12,567.    Elapsed: 0:08:41.\n",
      "  Batch 1,760  of  12,567.    Elapsed: 0:08:44.\n",
      "  Batch 1,770  of  12,567.    Elapsed: 0:08:47.\n",
      "  Batch 1,780  of  12,567.    Elapsed: 0:08:50.\n",
      "  Batch 1,790  of  12,567.    Elapsed: 0:08:53.\n",
      "  Batch 1,800  of  12,567.    Elapsed: 0:08:56.\n",
      "  Batch 1,810  of  12,567.    Elapsed: 0:08:59.\n",
      "  Batch 1,820  of  12,567.    Elapsed: 0:09:02.\n",
      "  Batch 1,830  of  12,567.    Elapsed: 0:09:05.\n",
      "  Batch 1,840  of  12,567.    Elapsed: 0:09:08.\n",
      "  Batch 1,850  of  12,567.    Elapsed: 0:09:11.\n",
      "  Batch 1,860  of  12,567.    Elapsed: 0:09:14.\n",
      "  Batch 1,870  of  12,567.    Elapsed: 0:09:17.\n",
      "  Batch 1,880  of  12,567.    Elapsed: 0:09:20.\n",
      "  Batch 1,890  of  12,567.    Elapsed: 0:09:23.\n",
      "  Batch 1,900  of  12,567.    Elapsed: 0:09:26.\n",
      "  Batch 1,910  of  12,567.    Elapsed: 0:09:29.\n",
      "  Batch 1,920  of  12,567.    Elapsed: 0:09:32.\n",
      "  Batch 1,930  of  12,567.    Elapsed: 0:09:34.\n",
      "  Batch 1,940  of  12,567.    Elapsed: 0:09:37.\n",
      "  Batch 1,950  of  12,567.    Elapsed: 0:09:40.\n",
      "  Batch 1,960  of  12,567.    Elapsed: 0:09:43.\n",
      "  Batch 1,970  of  12,567.    Elapsed: 0:09:46.\n",
      "  Batch 1,980  of  12,567.    Elapsed: 0:09:49.\n",
      "  Batch 1,990  of  12,567.    Elapsed: 0:09:52.\n",
      "  Batch 2,000  of  12,567.    Elapsed: 0:09:55.\n",
      "  Batch 2,010  of  12,567.    Elapsed: 0:09:58.\n",
      "  Batch 2,020  of  12,567.    Elapsed: 0:10:01.\n",
      "  Batch 2,030  of  12,567.    Elapsed: 0:10:04.\n",
      "  Batch 2,040  of  12,567.    Elapsed: 0:10:07.\n",
      "  Batch 2,050  of  12,567.    Elapsed: 0:10:10.\n",
      "  Batch 2,060  of  12,567.    Elapsed: 0:10:13.\n",
      "  Batch 2,070  of  12,567.    Elapsed: 0:10:16.\n",
      "  Batch 2,080  of  12,567.    Elapsed: 0:10:19.\n",
      "  Batch 2,090  of  12,567.    Elapsed: 0:10:22.\n",
      "  Batch 2,100  of  12,567.    Elapsed: 0:10:25.\n",
      "  Batch 2,110  of  12,567.    Elapsed: 0:10:28.\n",
      "  Batch 2,120  of  12,567.    Elapsed: 0:10:31.\n",
      "  Batch 2,130  of  12,567.    Elapsed: 0:10:34.\n",
      "  Batch 2,140  of  12,567.    Elapsed: 0:10:37.\n",
      "  Batch 2,150  of  12,567.    Elapsed: 0:10:40.\n",
      "  Batch 2,160  of  12,567.    Elapsed: 0:10:43.\n",
      "  Batch 2,170  of  12,567.    Elapsed: 0:10:46.\n",
      "  Batch 2,180  of  12,567.    Elapsed: 0:10:49.\n",
      "  Batch 2,190  of  12,567.    Elapsed: 0:10:52.\n",
      "  Batch 2,200  of  12,567.    Elapsed: 0:10:55.\n",
      "  Batch 2,210  of  12,567.    Elapsed: 0:10:58.\n",
      "  Batch 2,220  of  12,567.    Elapsed: 0:11:01.\n",
      "  Batch 2,230  of  12,567.    Elapsed: 0:11:04.\n",
      "  Batch 2,240  of  12,567.    Elapsed: 0:11:07.\n",
      "  Batch 2,250  of  12,567.    Elapsed: 0:11:10.\n",
      "  Batch 2,260  of  12,567.    Elapsed: 0:11:13.\n",
      "  Batch 2,270  of  12,567.    Elapsed: 0:11:16.\n",
      "  Batch 2,280  of  12,567.    Elapsed: 0:11:19.\n",
      "  Batch 2,290  of  12,567.    Elapsed: 0:11:22.\n",
      "  Batch 2,300  of  12,567.    Elapsed: 0:11:25.\n",
      "  Batch 2,310  of  12,567.    Elapsed: 0:11:28.\n",
      "  Batch 2,320  of  12,567.    Elapsed: 0:11:30.\n",
      "  Batch 2,330  of  12,567.    Elapsed: 0:11:33.\n",
      "  Batch 2,340  of  12,567.    Elapsed: 0:11:36.\n",
      "  Batch 2,350  of  12,567.    Elapsed: 0:11:39.\n",
      "  Batch 2,360  of  12,567.    Elapsed: 0:11:42.\n",
      "  Batch 2,370  of  12,567.    Elapsed: 0:11:45.\n",
      "  Batch 2,380  of  12,567.    Elapsed: 0:11:48.\n",
      "  Batch 2,390  of  12,567.    Elapsed: 0:11:51.\n",
      "  Batch 2,400  of  12,567.    Elapsed: 0:11:54.\n",
      "  Batch 2,410  of  12,567.    Elapsed: 0:11:58.\n",
      "  Batch 2,420  of  12,567.    Elapsed: 0:12:00.\n",
      "  Batch 2,430  of  12,567.    Elapsed: 0:12:03.\n",
      "  Batch 2,440  of  12,567.    Elapsed: 0:12:06.\n",
      "  Batch 2,450  of  12,567.    Elapsed: 0:12:09.\n",
      "  Batch 2,460  of  12,567.    Elapsed: 0:12:12.\n",
      "  Batch 2,470  of  12,567.    Elapsed: 0:12:15.\n",
      "  Batch 2,480  of  12,567.    Elapsed: 0:12:18.\n",
      "  Batch 2,490  of  12,567.    Elapsed: 0:12:21.\n",
      "  Batch 2,500  of  12,567.    Elapsed: 0:12:24.\n",
      "  Batch 2,510  of  12,567.    Elapsed: 0:12:27.\n",
      "  Batch 2,520  of  12,567.    Elapsed: 0:12:30.\n",
      "  Batch 2,530  of  12,567.    Elapsed: 0:12:33.\n",
      "  Batch 2,540  of  12,567.    Elapsed: 0:12:36.\n",
      "  Batch 2,550  of  12,567.    Elapsed: 0:12:39.\n",
      "  Batch 2,560  of  12,567.    Elapsed: 0:12:42.\n",
      "  Batch 2,570  of  12,567.    Elapsed: 0:12:45.\n",
      "  Batch 2,580  of  12,567.    Elapsed: 0:12:48.\n",
      "  Batch 2,590  of  12,567.    Elapsed: 0:12:51.\n",
      "  Batch 2,600  of  12,567.    Elapsed: 0:12:54.\n",
      "  Batch 2,610  of  12,567.    Elapsed: 0:12:57.\n",
      "  Batch 2,620  of  12,567.    Elapsed: 0:13:00.\n",
      "  Batch 2,630  of  12,567.    Elapsed: 0:13:03.\n",
      "  Batch 2,640  of  12,567.    Elapsed: 0:13:06.\n",
      "  Batch 2,650  of  12,567.    Elapsed: 0:13:09.\n",
      "  Batch 2,660  of  12,567.    Elapsed: 0:13:12.\n",
      "  Batch 2,670  of  12,567.    Elapsed: 0:13:15.\n",
      "  Batch 2,680  of  12,567.    Elapsed: 0:13:18.\n",
      "  Batch 2,690  of  12,567.    Elapsed: 0:13:21.\n",
      "  Batch 2,700  of  12,567.    Elapsed: 0:13:24.\n",
      "  Batch 2,710  of  12,567.    Elapsed: 0:13:27.\n",
      "  Batch 2,720  of  12,567.    Elapsed: 0:13:30.\n",
      "  Batch 2,730  of  12,567.    Elapsed: 0:13:33.\n",
      "  Batch 2,740  of  12,567.    Elapsed: 0:13:36.\n",
      "  Batch 2,750  of  12,567.    Elapsed: 0:13:39.\n",
      "  Batch 2,760  of  12,567.    Elapsed: 0:13:42.\n",
      "  Batch 2,770  of  12,567.    Elapsed: 0:13:45.\n",
      "  Batch 2,780  of  12,567.    Elapsed: 0:13:48.\n",
      "  Batch 2,790  of  12,567.    Elapsed: 0:13:51.\n",
      "  Batch 2,800  of  12,567.    Elapsed: 0:13:54.\n",
      "  Batch 2,810  of  12,567.    Elapsed: 0:13:57.\n",
      "  Batch 2,820  of  12,567.    Elapsed: 0:14:00.\n",
      "  Batch 2,830  of  12,567.    Elapsed: 0:14:03.\n",
      "  Batch 2,840  of  12,567.    Elapsed: 0:14:06.\n",
      "  Batch 2,850  of  12,567.    Elapsed: 0:14:09.\n",
      "  Batch 2,860  of  12,567.    Elapsed: 0:14:12.\n",
      "  Batch 2,870  of  12,567.    Elapsed: 0:14:15.\n",
      "  Batch 2,880  of  12,567.    Elapsed: 0:14:18.\n",
      "  Batch 2,890  of  12,567.    Elapsed: 0:14:21.\n",
      "  Batch 2,900  of  12,567.    Elapsed: 0:14:24.\n",
      "  Batch 2,910  of  12,567.    Elapsed: 0:14:27.\n",
      "  Batch 2,920  of  12,567.    Elapsed: 0:14:30.\n",
      "  Batch 2,930  of  12,567.    Elapsed: 0:14:33.\n",
      "  Batch 2,940  of  12,567.    Elapsed: 0:14:36.\n",
      "  Batch 2,950  of  12,567.    Elapsed: 0:14:38.\n",
      "  Batch 2,960  of  12,567.    Elapsed: 0:14:41.\n",
      "  Batch 2,970  of  12,567.    Elapsed: 0:14:44.\n",
      "  Batch 2,980  of  12,567.    Elapsed: 0:14:47.\n",
      "  Batch 2,990  of  12,567.    Elapsed: 0:14:50.\n",
      "  Batch 3,000  of  12,567.    Elapsed: 0:14:53.\n",
      "  Batch 3,010  of  12,567.    Elapsed: 0:14:56.\n",
      "  Batch 3,020  of  12,567.    Elapsed: 0:14:59.\n",
      "  Batch 3,030  of  12,567.    Elapsed: 0:15:03.\n",
      "  Batch 3,040  of  12,567.    Elapsed: 0:15:06.\n",
      "  Batch 3,050  of  12,567.    Elapsed: 0:15:09.\n",
      "  Batch 3,060  of  12,567.    Elapsed: 0:15:12.\n",
      "  Batch 3,070  of  12,567.    Elapsed: 0:15:15.\n",
      "  Batch 3,080  of  12,567.    Elapsed: 0:15:18.\n",
      "  Batch 3,090  of  12,567.    Elapsed: 0:15:21.\n",
      "  Batch 3,100  of  12,567.    Elapsed: 0:15:24.\n",
      "  Batch 3,110  of  12,567.    Elapsed: 0:15:27.\n",
      "  Batch 3,120  of  12,567.    Elapsed: 0:15:30.\n",
      "  Batch 3,130  of  12,567.    Elapsed: 0:15:33.\n",
      "  Batch 3,140  of  12,567.    Elapsed: 0:15:36.\n",
      "  Batch 3,150  of  12,567.    Elapsed: 0:15:39.\n",
      "  Batch 3,160  of  12,567.    Elapsed: 0:15:42.\n",
      "  Batch 3,170  of  12,567.    Elapsed: 0:15:45.\n",
      "  Batch 3,180  of  12,567.    Elapsed: 0:15:48.\n",
      "  Batch 3,190  of  12,567.    Elapsed: 0:15:51.\n",
      "  Batch 3,200  of  12,567.    Elapsed: 0:15:54.\n",
      "  Batch 3,210  of  12,567.    Elapsed: 0:15:57.\n",
      "  Batch 3,220  of  12,567.    Elapsed: 0:16:00.\n",
      "  Batch 3,230  of  12,567.    Elapsed: 0:16:04.\n",
      "  Batch 3,240  of  12,567.    Elapsed: 0:16:07.\n",
      "  Batch 3,250  of  12,567.    Elapsed: 0:16:10.\n",
      "  Batch 3,260  of  12,567.    Elapsed: 0:16:13.\n",
      "  Batch 3,270  of  12,567.    Elapsed: 0:16:16.\n",
      "  Batch 3,280  of  12,567.    Elapsed: 0:16:19.\n",
      "  Batch 3,290  of  12,567.    Elapsed: 0:16:22.\n",
      "  Batch 3,300  of  12,567.    Elapsed: 0:16:25.\n",
      "  Batch 3,310  of  12,567.    Elapsed: 0:16:28.\n",
      "  Batch 3,320  of  12,567.    Elapsed: 0:16:31.\n",
      "  Batch 3,330  of  12,567.    Elapsed: 0:16:34.\n",
      "  Batch 3,340  of  12,567.    Elapsed: 0:16:37.\n",
      "  Batch 3,350  of  12,567.    Elapsed: 0:16:40.\n",
      "  Batch 3,360  of  12,567.    Elapsed: 0:16:43.\n",
      "  Batch 3,370  of  12,567.    Elapsed: 0:16:46.\n",
      "  Batch 3,380  of  12,567.    Elapsed: 0:16:49.\n",
      "  Batch 3,390  of  12,567.    Elapsed: 0:16:52.\n",
      "  Batch 3,400  of  12,567.    Elapsed: 0:16:55.\n",
      "  Batch 3,410  of  12,567.    Elapsed: 0:16:58.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,420  of  12,567.    Elapsed: 0:17:01.\n",
      "  Batch 3,430  of  12,567.    Elapsed: 0:17:04.\n",
      "  Batch 3,440  of  12,567.    Elapsed: 0:17:07.\n",
      "  Batch 3,450  of  12,567.    Elapsed: 0:17:10.\n",
      "  Batch 3,460  of  12,567.    Elapsed: 0:17:13.\n",
      "  Batch 3,470  of  12,567.    Elapsed: 0:17:16.\n",
      "  Batch 3,480  of  12,567.    Elapsed: 0:17:19.\n",
      "  Batch 3,490  of  12,567.    Elapsed: 0:17:22.\n",
      "  Batch 3,500  of  12,567.    Elapsed: 0:17:25.\n",
      "  Batch 3,510  of  12,567.    Elapsed: 0:17:28.\n",
      "  Batch 3,520  of  12,567.    Elapsed: 0:17:31.\n",
      "  Batch 3,530  of  12,567.    Elapsed: 0:17:33.\n",
      "  Batch 3,540  of  12,567.    Elapsed: 0:17:36.\n",
      "  Batch 3,550  of  12,567.    Elapsed: 0:17:39.\n",
      "  Batch 3,560  of  12,567.    Elapsed: 0:17:42.\n",
      "  Batch 3,570  of  12,567.    Elapsed: 0:17:45.\n",
      "  Batch 3,580  of  12,567.    Elapsed: 0:17:48.\n",
      "  Batch 3,590  of  12,567.    Elapsed: 0:17:51.\n",
      "  Batch 3,600  of  12,567.    Elapsed: 0:17:54.\n",
      "  Batch 3,610  of  12,567.    Elapsed: 0:17:58.\n",
      "  Batch 3,620  of  12,567.    Elapsed: 0:18:00.\n",
      "  Batch 3,630  of  12,567.    Elapsed: 0:18:03.\n",
      "  Batch 3,640  of  12,567.    Elapsed: 0:18:06.\n",
      "  Batch 3,650  of  12,567.    Elapsed: 0:18:09.\n",
      "  Batch 3,660  of  12,567.    Elapsed: 0:18:12.\n",
      "  Batch 3,670  of  12,567.    Elapsed: 0:18:15.\n",
      "  Batch 3,680  of  12,567.    Elapsed: 0:18:18.\n",
      "  Batch 3,690  of  12,567.    Elapsed: 0:18:21.\n",
      "  Batch 3,700  of  12,567.    Elapsed: 0:18:24.\n",
      "  Batch 3,710  of  12,567.    Elapsed: 0:18:27.\n",
      "  Batch 3,720  of  12,567.    Elapsed: 0:18:30.\n",
      "  Batch 3,730  of  12,567.    Elapsed: 0:18:33.\n",
      "  Batch 3,740  of  12,567.    Elapsed: 0:18:36.\n",
      "  Batch 3,750  of  12,567.    Elapsed: 0:18:39.\n",
      "  Batch 3,760  of  12,567.    Elapsed: 0:18:42.\n",
      "  Batch 3,770  of  12,567.    Elapsed: 0:18:45.\n",
      "  Batch 3,780  of  12,567.    Elapsed: 0:18:48.\n",
      "  Batch 3,790  of  12,567.    Elapsed: 0:18:51.\n",
      "  Batch 3,800  of  12,567.    Elapsed: 0:18:54.\n",
      "  Batch 3,810  of  12,567.    Elapsed: 0:18:57.\n",
      "  Batch 3,820  of  12,567.    Elapsed: 0:19:00.\n",
      "  Batch 3,830  of  12,567.    Elapsed: 0:19:03.\n",
      "  Batch 3,840  of  12,567.    Elapsed: 0:19:06.\n",
      "  Batch 3,850  of  12,567.    Elapsed: 0:19:09.\n",
      "  Batch 3,860  of  12,567.    Elapsed: 0:19:12.\n",
      "  Batch 3,870  of  12,567.    Elapsed: 0:19:15.\n",
      "  Batch 3,880  of  12,567.    Elapsed: 0:19:18.\n",
      "  Batch 3,890  of  12,567.    Elapsed: 0:19:21.\n",
      "  Batch 3,900  of  12,567.    Elapsed: 0:19:24.\n",
      "  Batch 3,910  of  12,567.    Elapsed: 0:19:27.\n",
      "  Batch 3,920  of  12,567.    Elapsed: 0:19:30.\n",
      "  Batch 3,930  of  12,567.    Elapsed: 0:19:33.\n",
      "  Batch 3,940  of  12,567.    Elapsed: 0:19:36.\n",
      "  Batch 3,950  of  12,567.    Elapsed: 0:19:39.\n",
      "  Batch 3,960  of  12,567.    Elapsed: 0:19:42.\n",
      "  Batch 3,970  of  12,567.    Elapsed: 0:19:45.\n",
      "  Batch 3,980  of  12,567.    Elapsed: 0:19:48.\n",
      "  Batch 3,990  of  12,567.    Elapsed: 0:19:51.\n",
      "  Batch 4,000  of  12,567.    Elapsed: 0:19:54.\n",
      "  Batch 4,010  of  12,567.    Elapsed: 0:19:57.\n",
      "  Batch 4,020  of  12,567.    Elapsed: 0:20:00.\n",
      "  Batch 4,030  of  12,567.    Elapsed: 0:20:03.\n",
      "  Batch 4,040  of  12,567.    Elapsed: 0:20:06.\n",
      "  Batch 4,050  of  12,567.    Elapsed: 0:20:09.\n",
      "  Batch 4,060  of  12,567.    Elapsed: 0:20:12.\n",
      "  Batch 4,070  of  12,567.    Elapsed: 0:20:15.\n",
      "  Batch 4,080  of  12,567.    Elapsed: 0:20:18.\n",
      "  Batch 4,090  of  12,567.    Elapsed: 0:20:21.\n",
      "  Batch 4,100  of  12,567.    Elapsed: 0:20:24.\n",
      "  Batch 4,110  of  12,567.    Elapsed: 0:20:27.\n",
      "  Batch 4,120  of  12,567.    Elapsed: 0:20:30.\n",
      "  Batch 4,130  of  12,567.    Elapsed: 0:20:33.\n",
      "  Batch 4,140  of  12,567.    Elapsed: 0:20:36.\n",
      "  Batch 4,150  of  12,567.    Elapsed: 0:20:39.\n",
      "  Batch 4,160  of  12,567.    Elapsed: 0:20:42.\n",
      "  Batch 4,170  of  12,567.    Elapsed: 0:20:45.\n",
      "  Batch 4,180  of  12,567.    Elapsed: 0:20:48.\n",
      "  Batch 4,190  of  12,567.    Elapsed: 0:20:51.\n",
      "  Batch 4,200  of  12,567.    Elapsed: 0:20:53.\n",
      "  Batch 4,210  of  12,567.    Elapsed: 0:20:56.\n",
      "  Batch 4,220  of  12,567.    Elapsed: 0:20:59.\n",
      "  Batch 4,230  of  12,567.    Elapsed: 0:21:02.\n",
      "  Batch 4,240  of  12,567.    Elapsed: 0:21:05.\n",
      "  Batch 4,250  of  12,567.    Elapsed: 0:21:08.\n",
      "  Batch 4,260  of  12,567.    Elapsed: 0:21:11.\n",
      "  Batch 4,270  of  12,567.    Elapsed: 0:21:14.\n",
      "  Batch 4,280  of  12,567.    Elapsed: 0:21:17.\n",
      "  Batch 4,290  of  12,567.    Elapsed: 0:21:20.\n",
      "  Batch 4,300  of  12,567.    Elapsed: 0:21:23.\n",
      "  Batch 4,310  of  12,567.    Elapsed: 0:21:26.\n",
      "  Batch 4,320  of  12,567.    Elapsed: 0:21:29.\n",
      "  Batch 4,330  of  12,567.    Elapsed: 0:21:32.\n",
      "  Batch 4,340  of  12,567.    Elapsed: 0:21:35.\n",
      "  Batch 4,350  of  12,567.    Elapsed: 0:21:38.\n",
      "  Batch 4,360  of  12,567.    Elapsed: 0:21:41.\n",
      "  Batch 4,370  of  12,567.    Elapsed: 0:21:44.\n",
      "  Batch 4,380  of  12,567.    Elapsed: 0:21:47.\n",
      "  Batch 4,390  of  12,567.    Elapsed: 0:21:50.\n",
      "  Batch 4,400  of  12,567.    Elapsed: 0:21:53.\n",
      "  Batch 4,410  of  12,567.    Elapsed: 0:21:56.\n",
      "  Batch 4,420  of  12,567.    Elapsed: 0:21:59.\n",
      "  Batch 4,430  of  12,567.    Elapsed: 0:22:02.\n",
      "  Batch 4,440  of  12,567.    Elapsed: 0:22:05.\n",
      "  Batch 4,450  of  12,567.    Elapsed: 0:22:08.\n",
      "  Batch 4,460  of  12,567.    Elapsed: 0:22:11.\n",
      "  Batch 4,470  of  12,567.    Elapsed: 0:22:14.\n",
      "  Batch 4,480  of  12,567.    Elapsed: 0:22:17.\n",
      "  Batch 4,490  of  12,567.    Elapsed: 0:22:20.\n",
      "  Batch 4,500  of  12,567.    Elapsed: 0:22:23.\n",
      "  Batch 4,510  of  12,567.    Elapsed: 0:22:26.\n",
      "  Batch 4,520  of  12,567.    Elapsed: 0:22:29.\n",
      "  Batch 4,530  of  12,567.    Elapsed: 0:22:32.\n",
      "  Batch 4,540  of  12,567.    Elapsed: 0:22:35.\n",
      "  Batch 4,550  of  12,567.    Elapsed: 0:22:38.\n",
      "  Batch 4,560  of  12,567.    Elapsed: 0:22:41.\n",
      "  Batch 4,570  of  12,567.    Elapsed: 0:22:44.\n",
      "  Batch 4,580  of  12,567.    Elapsed: 0:22:47.\n",
      "  Batch 4,590  of  12,567.    Elapsed: 0:22:50.\n",
      "  Batch 4,600  of  12,567.    Elapsed: 0:22:53.\n",
      "  Batch 4,610  of  12,567.    Elapsed: 0:22:56.\n",
      "  Batch 4,620  of  12,567.    Elapsed: 0:22:59.\n",
      "  Batch 4,630  of  12,567.    Elapsed: 0:23:02.\n",
      "  Batch 4,640  of  12,567.    Elapsed: 0:23:05.\n",
      "  Batch 4,650  of  12,567.    Elapsed: 0:23:08.\n",
      "  Batch 4,660  of  12,567.    Elapsed: 0:23:11.\n",
      "  Batch 4,670  of  12,567.    Elapsed: 0:23:14.\n",
      "  Batch 4,680  of  12,567.    Elapsed: 0:23:17.\n",
      "  Batch 4,690  of  12,567.    Elapsed: 0:23:20.\n",
      "  Batch 4,700  of  12,567.    Elapsed: 0:23:23.\n",
      "  Batch 4,710  of  12,567.    Elapsed: 0:23:26.\n",
      "  Batch 4,720  of  12,567.    Elapsed: 0:23:29.\n",
      "  Batch 4,730  of  12,567.    Elapsed: 0:23:32.\n",
      "  Batch 4,740  of  12,567.    Elapsed: 0:23:35.\n",
      "  Batch 4,750  of  12,567.    Elapsed: 0:23:38.\n",
      "  Batch 4,760  of  12,567.    Elapsed: 0:23:40.\n",
      "  Batch 4,770  of  12,567.    Elapsed: 0:23:44.\n",
      "  Batch 4,780  of  12,567.    Elapsed: 0:23:47.\n",
      "  Batch 4,790  of  12,567.    Elapsed: 0:23:49.\n",
      "  Batch 4,800  of  12,567.    Elapsed: 0:23:52.\n",
      "  Batch 4,810  of  12,567.    Elapsed: 0:23:55.\n",
      "  Batch 4,820  of  12,567.    Elapsed: 0:23:58.\n",
      "  Batch 4,830  of  12,567.    Elapsed: 0:24:01.\n",
      "  Batch 4,840  of  12,567.    Elapsed: 0:24:04.\n",
      "  Batch 4,850  of  12,567.    Elapsed: 0:24:07.\n",
      "  Batch 4,860  of  12,567.    Elapsed: 0:24:10.\n",
      "  Batch 4,870  of  12,567.    Elapsed: 0:24:13.\n",
      "  Batch 4,880  of  12,567.    Elapsed: 0:24:16.\n",
      "  Batch 4,890  of  12,567.    Elapsed: 0:24:19.\n",
      "  Batch 4,900  of  12,567.    Elapsed: 0:24:22.\n",
      "  Batch 4,910  of  12,567.    Elapsed: 0:24:25.\n",
      "  Batch 4,920  of  12,567.    Elapsed: 0:24:28.\n",
      "  Batch 4,930  of  12,567.    Elapsed: 0:24:31.\n",
      "  Batch 4,940  of  12,567.    Elapsed: 0:24:34.\n",
      "  Batch 4,950  of  12,567.    Elapsed: 0:24:37.\n",
      "  Batch 4,960  of  12,567.    Elapsed: 0:24:40.\n",
      "  Batch 4,970  of  12,567.    Elapsed: 0:24:43.\n",
      "  Batch 4,980  of  12,567.    Elapsed: 0:24:46.\n",
      "  Batch 4,990  of  12,567.    Elapsed: 0:24:49.\n",
      "  Batch 5,000  of  12,567.    Elapsed: 0:24:52.\n",
      "  Batch 5,010  of  12,567.    Elapsed: 0:24:55.\n",
      "  Batch 5,020  of  12,567.    Elapsed: 0:24:58.\n",
      "  Batch 5,030  of  12,567.    Elapsed: 0:25:01.\n",
      "  Batch 5,040  of  12,567.    Elapsed: 0:25:04.\n",
      "  Batch 5,050  of  12,567.    Elapsed: 0:25:07.\n",
      "  Batch 5,060  of  12,567.    Elapsed: 0:25:10.\n",
      "  Batch 5,070  of  12,567.    Elapsed: 0:25:13.\n",
      "  Batch 5,080  of  12,567.    Elapsed: 0:25:16.\n",
      "  Batch 5,090  of  12,567.    Elapsed: 0:25:19.\n",
      "  Batch 5,100  of  12,567.    Elapsed: 0:25:22.\n",
      "  Batch 5,110  of  12,567.    Elapsed: 0:25:25.\n",
      "  Batch 5,120  of  12,567.    Elapsed: 0:25:28.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5,130  of  12,567.    Elapsed: 0:25:31.\n",
      "  Batch 5,140  of  12,567.    Elapsed: 0:25:34.\n",
      "  Batch 5,150  of  12,567.    Elapsed: 0:25:37.\n",
      "  Batch 5,160  of  12,567.    Elapsed: 0:25:40.\n",
      "  Batch 5,170  of  12,567.    Elapsed: 0:25:43.\n",
      "  Batch 5,180  of  12,567.    Elapsed: 0:25:46.\n",
      "  Batch 5,190  of  12,567.    Elapsed: 0:25:49.\n",
      "  Batch 5,200  of  12,567.    Elapsed: 0:25:52.\n",
      "  Batch 5,210  of  12,567.    Elapsed: 0:25:54.\n",
      "  Batch 5,220  of  12,567.    Elapsed: 0:25:57.\n",
      "  Batch 5,230  of  12,567.    Elapsed: 0:26:00.\n",
      "  Batch 5,240  of  12,567.    Elapsed: 0:26:03.\n",
      "  Batch 5,250  of  12,567.    Elapsed: 0:26:06.\n",
      "  Batch 5,260  of  12,567.    Elapsed: 0:26:09.\n",
      "  Batch 5,270  of  12,567.    Elapsed: 0:26:12.\n",
      "  Batch 5,280  of  12,567.    Elapsed: 0:26:15.\n",
      "  Batch 5,290  of  12,567.    Elapsed: 0:26:18.\n",
      "  Batch 5,300  of  12,567.    Elapsed: 0:26:21.\n",
      "  Batch 5,310  of  12,567.    Elapsed: 0:26:24.\n",
      "  Batch 5,320  of  12,567.    Elapsed: 0:26:27.\n",
      "  Batch 5,330  of  12,567.    Elapsed: 0:26:30.\n",
      "  Batch 5,340  of  12,567.    Elapsed: 0:26:33.\n",
      "  Batch 5,350  of  12,567.    Elapsed: 0:26:36.\n",
      "  Batch 5,360  of  12,567.    Elapsed: 0:26:39.\n",
      "  Batch 5,370  of  12,567.    Elapsed: 0:26:42.\n",
      "  Batch 5,380  of  12,567.    Elapsed: 0:26:45.\n",
      "  Batch 5,390  of  12,567.    Elapsed: 0:26:48.\n",
      "  Batch 5,400  of  12,567.    Elapsed: 0:26:51.\n",
      "  Batch 5,410  of  12,567.    Elapsed: 0:26:54.\n",
      "  Batch 5,420  of  12,567.    Elapsed: 0:26:57.\n",
      "  Batch 5,430  of  12,567.    Elapsed: 0:27:00.\n",
      "  Batch 5,440  of  12,567.    Elapsed: 0:27:03.\n",
      "  Batch 5,450  of  12,567.    Elapsed: 0:27:06.\n",
      "  Batch 5,460  of  12,567.    Elapsed: 0:27:09.\n",
      "  Batch 5,470  of  12,567.    Elapsed: 0:27:12.\n",
      "  Batch 5,480  of  12,567.    Elapsed: 0:27:15.\n",
      "  Batch 5,490  of  12,567.    Elapsed: 0:27:18.\n",
      "  Batch 5,500  of  12,567.    Elapsed: 0:27:21.\n",
      "  Batch 5,510  of  12,567.    Elapsed: 0:27:24.\n",
      "  Batch 5,520  of  12,567.    Elapsed: 0:27:27.\n",
      "  Batch 5,530  of  12,567.    Elapsed: 0:27:30.\n",
      "  Batch 5,540  of  12,567.    Elapsed: 0:27:33.\n",
      "  Batch 5,550  of  12,567.    Elapsed: 0:27:36.\n",
      "  Batch 5,560  of  12,567.    Elapsed: 0:27:39.\n",
      "  Batch 5,570  of  12,567.    Elapsed: 0:27:42.\n",
      "  Batch 5,580  of  12,567.    Elapsed: 0:27:45.\n",
      "  Batch 5,590  of  12,567.    Elapsed: 0:27:48.\n",
      "  Batch 5,600  of  12,567.    Elapsed: 0:27:51.\n",
      "  Batch 5,610  of  12,567.    Elapsed: 0:27:54.\n",
      "  Batch 5,620  of  12,567.    Elapsed: 0:27:57.\n",
      "  Batch 5,630  of  12,567.    Elapsed: 0:27:59.\n",
      "  Batch 5,640  of  12,567.    Elapsed: 0:28:02.\n",
      "  Batch 5,650  of  12,567.    Elapsed: 0:28:05.\n",
      "  Batch 5,660  of  12,567.    Elapsed: 0:28:08.\n",
      "  Batch 5,670  of  12,567.    Elapsed: 0:28:11.\n",
      "  Batch 5,680  of  12,567.    Elapsed: 0:28:14.\n",
      "  Batch 5,690  of  12,567.    Elapsed: 0:28:17.\n",
      "  Batch 5,700  of  12,567.    Elapsed: 0:28:20.\n",
      "  Batch 5,710  of  12,567.    Elapsed: 0:28:23.\n",
      "  Batch 5,720  of  12,567.    Elapsed: 0:28:26.\n",
      "  Batch 5,730  of  12,567.    Elapsed: 0:28:29.\n",
      "  Batch 5,740  of  12,567.    Elapsed: 0:28:32.\n",
      "  Batch 5,750  of  12,567.    Elapsed: 0:28:35.\n",
      "  Batch 5,760  of  12,567.    Elapsed: 0:28:38.\n",
      "  Batch 5,770  of  12,567.    Elapsed: 0:28:41.\n",
      "  Batch 5,780  of  12,567.    Elapsed: 0:28:44.\n",
      "  Batch 5,790  of  12,567.    Elapsed: 0:28:47.\n",
      "  Batch 5,800  of  12,567.    Elapsed: 0:28:50.\n",
      "  Batch 5,810  of  12,567.    Elapsed: 0:28:53.\n",
      "  Batch 5,820  of  12,567.    Elapsed: 0:28:56.\n",
      "  Batch 5,830  of  12,567.    Elapsed: 0:28:59.\n",
      "  Batch 5,840  of  12,567.    Elapsed: 0:29:02.\n",
      "  Batch 5,850  of  12,567.    Elapsed: 0:29:05.\n",
      "  Batch 5,860  of  12,567.    Elapsed: 0:29:08.\n",
      "  Batch 5,870  of  12,567.    Elapsed: 0:29:11.\n",
      "  Batch 5,880  of  12,567.    Elapsed: 0:29:14.\n",
      "  Batch 5,890  of  12,567.    Elapsed: 0:29:17.\n",
      "  Batch 5,900  of  12,567.    Elapsed: 0:29:20.\n",
      "  Batch 5,910  of  12,567.    Elapsed: 0:29:23.\n",
      "  Batch 5,920  of  12,567.    Elapsed: 0:29:26.\n",
      "  Batch 5,930  of  12,567.    Elapsed: 0:29:29.\n",
      "  Batch 5,940  of  12,567.    Elapsed: 0:29:32.\n",
      "  Batch 5,950  of  12,567.    Elapsed: 0:29:35.\n",
      "  Batch 5,960  of  12,567.    Elapsed: 0:29:38.\n",
      "  Batch 5,970  of  12,567.    Elapsed: 0:29:41.\n",
      "  Batch 5,980  of  12,567.    Elapsed: 0:29:44.\n",
      "  Batch 5,990  of  12,567.    Elapsed: 0:29:47.\n",
      "  Batch 6,000  of  12,567.    Elapsed: 0:29:50.\n",
      "  Batch 6,010  of  12,567.    Elapsed: 0:29:53.\n",
      "  Batch 6,020  of  12,567.    Elapsed: 0:29:56.\n",
      "  Batch 6,030  of  12,567.    Elapsed: 0:29:59.\n",
      "  Batch 6,040  of  12,567.    Elapsed: 0:30:02.\n",
      "  Batch 6,050  of  12,567.    Elapsed: 0:30:05.\n",
      "  Batch 6,060  of  12,567.    Elapsed: 0:30:08.\n",
      "  Batch 6,070  of  12,567.    Elapsed: 0:30:11.\n",
      "  Batch 6,080  of  12,567.    Elapsed: 0:30:14.\n",
      "  Batch 6,090  of  12,567.    Elapsed: 0:30:17.\n",
      "  Batch 6,100  of  12,567.    Elapsed: 0:30:20.\n",
      "  Batch 6,110  of  12,567.    Elapsed: 0:30:23.\n",
      "  Batch 6,120  of  12,567.    Elapsed: 0:30:26.\n",
      "  Batch 6,130  of  12,567.    Elapsed: 0:30:29.\n",
      "  Batch 6,140  of  12,567.    Elapsed: 0:30:32.\n",
      "  Batch 6,150  of  12,567.    Elapsed: 0:30:35.\n",
      "  Batch 6,160  of  12,567.    Elapsed: 0:30:38.\n",
      "  Batch 6,170  of  12,567.    Elapsed: 0:30:40.\n",
      "  Batch 6,180  of  12,567.    Elapsed: 0:30:44.\n",
      "  Batch 6,190  of  12,567.    Elapsed: 0:30:47.\n",
      "  Batch 6,200  of  12,567.    Elapsed: 0:30:50.\n",
      "  Batch 6,210  of  12,567.    Elapsed: 0:30:52.\n",
      "  Batch 6,220  of  12,567.    Elapsed: 0:30:56.\n",
      "  Batch 6,230  of  12,567.    Elapsed: 0:30:58.\n",
      "  Batch 6,240  of  12,567.    Elapsed: 0:31:01.\n",
      "  Batch 6,250  of  12,567.    Elapsed: 0:31:04.\n",
      "  Batch 6,260  of  12,567.    Elapsed: 0:31:07.\n",
      "  Batch 6,270  of  12,567.    Elapsed: 0:31:10.\n",
      "  Batch 6,280  of  12,567.    Elapsed: 0:31:13.\n",
      "  Batch 6,290  of  12,567.    Elapsed: 0:31:16.\n",
      "  Batch 6,300  of  12,567.    Elapsed: 0:31:19.\n",
      "  Batch 6,310  of  12,567.    Elapsed: 0:31:22.\n",
      "  Batch 6,320  of  12,567.    Elapsed: 0:31:25.\n",
      "  Batch 6,330  of  12,567.    Elapsed: 0:31:28.\n",
      "  Batch 6,340  of  12,567.    Elapsed: 0:31:31.\n",
      "  Batch 6,350  of  12,567.    Elapsed: 0:31:34.\n",
      "  Batch 6,360  of  12,567.    Elapsed: 0:31:37.\n",
      "  Batch 6,370  of  12,567.    Elapsed: 0:31:40.\n",
      "  Batch 6,380  of  12,567.    Elapsed: 0:31:43.\n",
      "  Batch 6,390  of  12,567.    Elapsed: 0:31:46.\n",
      "  Batch 6,400  of  12,567.    Elapsed: 0:31:49.\n",
      "  Batch 6,410  of  12,567.    Elapsed: 0:31:52.\n",
      "  Batch 6,420  of  12,567.    Elapsed: 0:31:55.\n",
      "  Batch 6,430  of  12,567.    Elapsed: 0:31:58.\n",
      "  Batch 6,440  of  12,567.    Elapsed: 0:32:01.\n",
      "  Batch 6,450  of  12,567.    Elapsed: 0:32:04.\n",
      "  Batch 6,460  of  12,567.    Elapsed: 0:32:07.\n",
      "  Batch 6,470  of  12,567.    Elapsed: 0:32:10.\n",
      "  Batch 6,480  of  12,567.    Elapsed: 0:32:13.\n",
      "  Batch 6,490  of  12,567.    Elapsed: 0:32:16.\n",
      "  Batch 6,500  of  12,567.    Elapsed: 0:32:19.\n",
      "  Batch 6,510  of  12,567.    Elapsed: 0:32:22.\n",
      "  Batch 6,520  of  12,567.    Elapsed: 0:32:25.\n",
      "  Batch 6,530  of  12,567.    Elapsed: 0:32:28.\n",
      "  Batch 6,540  of  12,567.    Elapsed: 0:32:31.\n",
      "  Batch 6,550  of  12,567.    Elapsed: 0:32:34.\n",
      "  Batch 6,560  of  12,567.    Elapsed: 0:32:37.\n",
      "  Batch 6,570  of  12,567.    Elapsed: 0:32:40.\n",
      "  Batch 6,580  of  12,567.    Elapsed: 0:32:43.\n",
      "  Batch 6,590  of  12,567.    Elapsed: 0:32:46.\n",
      "  Batch 6,600  of  12,567.    Elapsed: 0:32:49.\n",
      "  Batch 6,610  of  12,567.    Elapsed: 0:32:52.\n",
      "  Batch 6,620  of  12,567.    Elapsed: 0:32:55.\n",
      "  Batch 6,630  of  12,567.    Elapsed: 0:32:58.\n",
      "  Batch 6,640  of  12,567.    Elapsed: 0:33:00.\n",
      "  Batch 6,650  of  12,567.    Elapsed: 0:33:03.\n",
      "  Batch 6,660  of  12,567.    Elapsed: 0:33:06.\n",
      "  Batch 6,670  of  12,567.    Elapsed: 0:33:09.\n",
      "  Batch 6,680  of  12,567.    Elapsed: 0:33:12.\n",
      "  Batch 6,690  of  12,567.    Elapsed: 0:33:15.\n",
      "  Batch 6,700  of  12,567.    Elapsed: 0:33:18.\n",
      "  Batch 6,710  of  12,567.    Elapsed: 0:33:21.\n",
      "  Batch 6,720  of  12,567.    Elapsed: 0:33:24.\n",
      "  Batch 6,730  of  12,567.    Elapsed: 0:33:27.\n",
      "  Batch 6,740  of  12,567.    Elapsed: 0:33:30.\n",
      "  Batch 6,750  of  12,567.    Elapsed: 0:33:33.\n",
      "  Batch 6,760  of  12,567.    Elapsed: 0:33:36.\n",
      "  Batch 6,770  of  12,567.    Elapsed: 0:33:39.\n",
      "  Batch 6,780  of  12,567.    Elapsed: 0:33:42.\n",
      "  Batch 6,790  of  12,567.    Elapsed: 0:33:45.\n",
      "  Batch 6,800  of  12,567.    Elapsed: 0:33:48.\n",
      "  Batch 6,810  of  12,567.    Elapsed: 0:33:51.\n",
      "  Batch 6,820  of  12,567.    Elapsed: 0:33:54.\n",
      "  Batch 6,830  of  12,567.    Elapsed: 0:33:57.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 6,840  of  12,567.    Elapsed: 0:34:00.\n",
      "  Batch 6,850  of  12,567.    Elapsed: 0:34:03.\n",
      "  Batch 6,860  of  12,567.    Elapsed: 0:34:06.\n",
      "  Batch 6,870  of  12,567.    Elapsed: 0:34:09.\n",
      "  Batch 6,880  of  12,567.    Elapsed: 0:34:12.\n",
      "  Batch 6,890  of  12,567.    Elapsed: 0:34:15.\n",
      "  Batch 6,900  of  12,567.    Elapsed: 0:34:18.\n",
      "  Batch 6,910  of  12,567.    Elapsed: 0:34:21.\n",
      "  Batch 6,920  of  12,567.    Elapsed: 0:34:24.\n",
      "  Batch 6,930  of  12,567.    Elapsed: 0:34:27.\n",
      "  Batch 6,940  of  12,567.    Elapsed: 0:34:30.\n",
      "  Batch 6,950  of  12,567.    Elapsed: 0:34:33.\n",
      "  Batch 6,960  of  12,567.    Elapsed: 0:34:36.\n",
      "  Batch 6,970  of  12,567.    Elapsed: 0:34:39.\n",
      "  Batch 6,980  of  12,567.    Elapsed: 0:34:42.\n",
      "  Batch 6,990  of  12,567.    Elapsed: 0:34:45.\n",
      "  Batch 7,000  of  12,567.    Elapsed: 0:34:48.\n",
      "  Batch 7,010  of  12,567.    Elapsed: 0:34:51.\n",
      "  Batch 7,020  of  12,567.    Elapsed: 0:34:54.\n",
      "  Batch 7,030  of  12,567.    Elapsed: 0:34:57.\n",
      "  Batch 7,040  of  12,567.    Elapsed: 0:35:00.\n",
      "  Batch 7,050  of  12,567.    Elapsed: 0:35:02.\n",
      "  Batch 7,060  of  12,567.    Elapsed: 0:35:05.\n",
      "  Batch 7,070  of  12,567.    Elapsed: 0:35:08.\n",
      "  Batch 7,080  of  12,567.    Elapsed: 0:35:11.\n",
      "  Batch 7,090  of  12,567.    Elapsed: 0:35:14.\n",
      "  Batch 7,100  of  12,567.    Elapsed: 0:35:17.\n",
      "  Batch 7,110  of  12,567.    Elapsed: 0:35:20.\n",
      "  Batch 7,120  of  12,567.    Elapsed: 0:35:23.\n",
      "  Batch 7,130  of  12,567.    Elapsed: 0:35:26.\n",
      "  Batch 7,140  of  12,567.    Elapsed: 0:35:29.\n",
      "  Batch 7,150  of  12,567.    Elapsed: 0:35:32.\n",
      "  Batch 7,160  of  12,567.    Elapsed: 0:35:35.\n",
      "  Batch 7,170  of  12,567.    Elapsed: 0:35:38.\n",
      "  Batch 7,180  of  12,567.    Elapsed: 0:35:41.\n",
      "  Batch 7,190  of  12,567.    Elapsed: 0:35:45.\n",
      "  Batch 7,200  of  12,567.    Elapsed: 0:35:48.\n",
      "  Batch 7,210  of  12,567.    Elapsed: 0:35:51.\n",
      "  Batch 7,220  of  12,567.    Elapsed: 0:35:54.\n",
      "  Batch 7,230  of  12,567.    Elapsed: 0:35:57.\n",
      "  Batch 7,240  of  12,567.    Elapsed: 0:36:00.\n",
      "  Batch 7,250  of  12,567.    Elapsed: 0:36:03.\n",
      "  Batch 7,260  of  12,567.    Elapsed: 0:36:06.\n",
      "  Batch 7,270  of  12,567.    Elapsed: 0:36:09.\n",
      "  Batch 7,280  of  12,567.    Elapsed: 0:36:12.\n",
      "  Batch 7,290  of  12,567.    Elapsed: 0:36:15.\n",
      "  Batch 7,300  of  12,567.    Elapsed: 0:36:18.\n",
      "  Batch 7,310  of  12,567.    Elapsed: 0:36:21.\n",
      "  Batch 7,320  of  12,567.    Elapsed: 0:36:25.\n",
      "  Batch 7,330  of  12,567.    Elapsed: 0:36:28.\n",
      "  Batch 7,340  of  12,567.    Elapsed: 0:36:31.\n",
      "  Batch 7,350  of  12,567.    Elapsed: 0:36:34.\n",
      "  Batch 7,360  of  12,567.    Elapsed: 0:36:37.\n",
      "  Batch 7,370  of  12,567.    Elapsed: 0:36:40.\n",
      "  Batch 7,380  of  12,567.    Elapsed: 0:36:43.\n",
      "  Batch 7,390  of  12,567.    Elapsed: 0:36:46.\n",
      "  Batch 7,400  of  12,567.    Elapsed: 0:36:49.\n",
      "  Batch 7,410  of  12,567.    Elapsed: 0:36:52.\n",
      "  Batch 7,420  of  12,567.    Elapsed: 0:36:55.\n",
      "  Batch 7,430  of  12,567.    Elapsed: 0:36:59.\n",
      "  Batch 7,440  of  12,567.    Elapsed: 0:37:02.\n",
      "  Batch 7,450  of  12,567.    Elapsed: 0:37:05.\n",
      "  Batch 7,460  of  12,567.    Elapsed: 0:37:08.\n",
      "  Batch 7,470  of  12,567.    Elapsed: 0:37:11.\n",
      "  Batch 7,480  of  12,567.    Elapsed: 0:37:14.\n",
      "  Batch 7,490  of  12,567.    Elapsed: 0:37:17.\n",
      "  Batch 7,500  of  12,567.    Elapsed: 0:37:20.\n",
      "  Batch 7,510  of  12,567.    Elapsed: 0:37:23.\n",
      "  Batch 7,520  of  12,567.    Elapsed: 0:37:26.\n",
      "  Batch 7,530  of  12,567.    Elapsed: 0:37:29.\n",
      "  Batch 7,540  of  12,567.    Elapsed: 0:37:32.\n",
      "  Batch 7,550  of  12,567.    Elapsed: 0:37:35.\n",
      "  Batch 7,560  of  12,567.    Elapsed: 0:37:38.\n",
      "  Batch 7,570  of  12,567.    Elapsed: 0:37:41.\n",
      "  Batch 7,580  of  12,567.    Elapsed: 0:37:44.\n",
      "  Batch 7,590  of  12,567.    Elapsed: 0:37:47.\n",
      "  Batch 7,600  of  12,567.    Elapsed: 0:37:50.\n",
      "  Batch 7,610  of  12,567.    Elapsed: 0:37:53.\n",
      "  Batch 7,620  of  12,567.    Elapsed: 0:37:56.\n",
      "  Batch 7,630  of  12,567.    Elapsed: 0:37:59.\n",
      "  Batch 7,640  of  12,567.    Elapsed: 0:38:02.\n",
      "  Batch 7,650  of  12,567.    Elapsed: 0:38:05.\n",
      "  Batch 7,660  of  12,567.    Elapsed: 0:38:08.\n",
      "  Batch 7,670  of  12,567.    Elapsed: 0:38:11.\n",
      "  Batch 7,680  of  12,567.    Elapsed: 0:38:14.\n",
      "  Batch 7,690  of  12,567.    Elapsed: 0:38:17.\n",
      "  Batch 7,700  of  12,567.    Elapsed: 0:38:20.\n",
      "  Batch 7,710  of  12,567.    Elapsed: 0:38:23.\n",
      "  Batch 7,720  of  12,567.    Elapsed: 0:38:26.\n",
      "  Batch 7,730  of  12,567.    Elapsed: 0:38:29.\n",
      "  Batch 7,740  of  12,567.    Elapsed: 0:38:32.\n",
      "  Batch 7,750  of  12,567.    Elapsed: 0:38:35.\n",
      "  Batch 7,760  of  12,567.    Elapsed: 0:38:38.\n",
      "  Batch 7,770  of  12,567.    Elapsed: 0:38:41.\n",
      "  Batch 7,780  of  12,567.    Elapsed: 0:38:44.\n",
      "  Batch 7,790  of  12,567.    Elapsed: 0:38:47.\n",
      "  Batch 7,800  of  12,567.    Elapsed: 0:38:50.\n",
      "  Batch 7,810  of  12,567.    Elapsed: 0:38:53.\n",
      "  Batch 7,820  of  12,567.    Elapsed: 0:38:56.\n",
      "  Batch 7,830  of  12,567.    Elapsed: 0:38:59.\n",
      "  Batch 7,840  of  12,567.    Elapsed: 0:39:02.\n",
      "  Batch 7,850  of  12,567.    Elapsed: 0:39:04.\n",
      "  Batch 7,860  of  12,567.    Elapsed: 0:39:07.\n",
      "  Batch 7,870  of  12,567.    Elapsed: 0:39:10.\n",
      "  Batch 7,880  of  12,567.    Elapsed: 0:39:13.\n",
      "  Batch 7,890  of  12,567.    Elapsed: 0:39:16.\n",
      "  Batch 7,900  of  12,567.    Elapsed: 0:39:19.\n",
      "  Batch 7,910  of  12,567.    Elapsed: 0:39:22.\n",
      "  Batch 7,920  of  12,567.    Elapsed: 0:39:25.\n",
      "  Batch 7,930  of  12,567.    Elapsed: 0:39:28.\n",
      "  Batch 7,940  of  12,567.    Elapsed: 0:39:31.\n",
      "  Batch 7,950  of  12,567.    Elapsed: 0:39:34.\n",
      "  Batch 7,960  of  12,567.    Elapsed: 0:39:37.\n",
      "  Batch 7,970  of  12,567.    Elapsed: 0:39:40.\n",
      "  Batch 7,980  of  12,567.    Elapsed: 0:39:43.\n",
      "  Batch 7,990  of  12,567.    Elapsed: 0:39:46.\n",
      "  Batch 8,000  of  12,567.    Elapsed: 0:39:49.\n",
      "  Batch 8,010  of  12,567.    Elapsed: 0:39:52.\n",
      "  Batch 8,020  of  12,567.    Elapsed: 0:39:55.\n",
      "  Batch 8,030  of  12,567.    Elapsed: 0:39:58.\n",
      "  Batch 8,040  of  12,567.    Elapsed: 0:40:01.\n",
      "  Batch 8,050  of  12,567.    Elapsed: 0:40:04.\n",
      "  Batch 8,060  of  12,567.    Elapsed: 0:40:07.\n",
      "  Batch 8,070  of  12,567.    Elapsed: 0:40:10.\n",
      "  Batch 8,080  of  12,567.    Elapsed: 0:40:13.\n",
      "  Batch 8,090  of  12,567.    Elapsed: 0:40:16.\n",
      "  Batch 8,100  of  12,567.    Elapsed: 0:40:19.\n",
      "  Batch 8,110  of  12,567.    Elapsed: 0:40:22.\n",
      "  Batch 8,120  of  12,567.    Elapsed: 0:40:25.\n",
      "  Batch 8,130  of  12,567.    Elapsed: 0:40:28.\n",
      "  Batch 8,140  of  12,567.    Elapsed: 0:40:31.\n",
      "  Batch 8,150  of  12,567.    Elapsed: 0:40:34.\n",
      "  Batch 8,160  of  12,567.    Elapsed: 0:40:37.\n",
      "  Batch 8,170  of  12,567.    Elapsed: 0:40:40.\n",
      "  Batch 8,180  of  12,567.    Elapsed: 0:40:43.\n",
      "  Batch 8,190  of  12,567.    Elapsed: 0:40:46.\n",
      "  Batch 8,200  of  12,567.    Elapsed: 0:40:49.\n",
      "  Batch 8,210  of  12,567.    Elapsed: 0:40:52.\n",
      "  Batch 8,220  of  12,567.    Elapsed: 0:40:55.\n",
      "  Batch 8,230  of  12,567.    Elapsed: 0:40:58.\n",
      "  Batch 8,240  of  12,567.    Elapsed: 0:41:01.\n",
      "  Batch 8,250  of  12,567.    Elapsed: 0:41:04.\n",
      "  Batch 8,260  of  12,567.    Elapsed: 0:41:06.\n",
      "  Batch 8,270  of  12,567.    Elapsed: 0:41:09.\n",
      "  Batch 8,280  of  12,567.    Elapsed: 0:41:12.\n",
      "  Batch 8,290  of  12,567.    Elapsed: 0:41:15.\n",
      "  Batch 8,300  of  12,567.    Elapsed: 0:41:18.\n",
      "  Batch 8,310  of  12,567.    Elapsed: 0:41:21.\n",
      "  Batch 8,320  of  12,567.    Elapsed: 0:41:24.\n",
      "  Batch 8,330  of  12,567.    Elapsed: 0:41:27.\n",
      "  Batch 8,340  of  12,567.    Elapsed: 0:41:30.\n",
      "  Batch 8,350  of  12,567.    Elapsed: 0:41:33.\n",
      "  Batch 8,360  of  12,567.    Elapsed: 0:41:36.\n",
      "  Batch 8,370  of  12,567.    Elapsed: 0:41:39.\n",
      "  Batch 8,380  of  12,567.    Elapsed: 0:41:42.\n",
      "  Batch 8,390  of  12,567.    Elapsed: 0:41:45.\n",
      "  Batch 8,400  of  12,567.    Elapsed: 0:41:48.\n",
      "  Batch 8,410  of  12,567.    Elapsed: 0:41:51.\n",
      "  Batch 8,420  of  12,567.    Elapsed: 0:41:54.\n",
      "  Batch 8,430  of  12,567.    Elapsed: 0:41:57.\n",
      "  Batch 8,440  of  12,567.    Elapsed: 0:42:00.\n",
      "  Batch 8,450  of  12,567.    Elapsed: 0:42:03.\n",
      "  Batch 8,460  of  12,567.    Elapsed: 0:42:06.\n",
      "  Batch 8,470  of  12,567.    Elapsed: 0:42:09.\n",
      "  Batch 8,480  of  12,567.    Elapsed: 0:42:12.\n",
      "  Batch 8,490  of  12,567.    Elapsed: 0:42:15.\n",
      "  Batch 8,500  of  12,567.    Elapsed: 0:42:18.\n",
      "  Batch 8,510  of  12,567.    Elapsed: 0:42:21.\n",
      "  Batch 8,520  of  12,567.    Elapsed: 0:42:24.\n",
      "  Batch 8,530  of  12,567.    Elapsed: 0:42:27.\n",
      "  Batch 8,540  of  12,567.    Elapsed: 0:42:30.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 8,550  of  12,567.    Elapsed: 0:42:33.\n",
      "  Batch 8,560  of  12,567.    Elapsed: 0:42:36.\n",
      "  Batch 8,570  of  12,567.    Elapsed: 0:42:39.\n",
      "  Batch 8,580  of  12,567.    Elapsed: 0:42:42.\n",
      "  Batch 8,590  of  12,567.    Elapsed: 0:42:46.\n",
      "  Batch 8,600  of  12,567.    Elapsed: 0:42:49.\n",
      "  Batch 8,610  of  12,567.    Elapsed: 0:42:52.\n",
      "  Batch 8,620  of  12,567.    Elapsed: 0:42:55.\n",
      "  Batch 8,630  of  12,567.    Elapsed: 0:42:58.\n",
      "  Batch 8,640  of  12,567.    Elapsed: 0:43:01.\n",
      "  Batch 8,650  of  12,567.    Elapsed: 0:43:04.\n",
      "  Batch 8,660  of  12,567.    Elapsed: 0:43:07.\n",
      "  Batch 8,670  of  12,567.    Elapsed: 0:43:10.\n",
      "  Batch 8,680  of  12,567.    Elapsed: 0:43:13.\n",
      "  Batch 8,690  of  12,567.    Elapsed: 0:43:16.\n",
      "  Batch 8,700  of  12,567.    Elapsed: 0:43:19.\n",
      "  Batch 8,710  of  12,567.    Elapsed: 0:43:22.\n",
      "  Batch 8,720  of  12,567.    Elapsed: 0:43:25.\n",
      "  Batch 8,730  of  12,567.    Elapsed: 0:43:28.\n",
      "  Batch 8,740  of  12,567.    Elapsed: 0:43:31.\n",
      "  Batch 8,750  of  12,567.    Elapsed: 0:43:34.\n",
      "  Batch 8,760  of  12,567.    Elapsed: 0:43:37.\n",
      "  Batch 8,770  of  12,567.    Elapsed: 0:43:40.\n",
      "  Batch 8,780  of  12,567.    Elapsed: 0:43:43.\n",
      "  Batch 8,790  of  12,567.    Elapsed: 0:43:46.\n",
      "  Batch 8,800  of  12,567.    Elapsed: 0:43:49.\n",
      "  Batch 8,810  of  12,567.    Elapsed: 0:43:52.\n",
      "  Batch 8,820  of  12,567.    Elapsed: 0:43:55.\n",
      "  Batch 8,830  of  12,567.    Elapsed: 0:43:58.\n",
      "  Batch 8,840  of  12,567.    Elapsed: 0:44:01.\n",
      "  Batch 8,850  of  12,567.    Elapsed: 0:44:04.\n",
      "  Batch 8,860  of  12,567.    Elapsed: 0:44:07.\n",
      "  Batch 8,870  of  12,567.    Elapsed: 0:44:10.\n",
      "  Batch 8,880  of  12,567.    Elapsed: 0:44:13.\n",
      "  Batch 8,890  of  12,567.    Elapsed: 0:44:16.\n",
      "  Batch 8,900  of  12,567.    Elapsed: 0:44:19.\n",
      "  Batch 8,910  of  12,567.    Elapsed: 0:44:22.\n",
      "  Batch 8,920  of  12,567.    Elapsed: 0:44:25.\n",
      "  Batch 8,930  of  12,567.    Elapsed: 0:44:28.\n",
      "  Batch 8,940  of  12,567.    Elapsed: 0:44:32.\n",
      "  Batch 8,950  of  12,567.    Elapsed: 0:44:35.\n",
      "  Batch 8,960  of  12,567.    Elapsed: 0:44:38.\n",
      "  Batch 8,970  of  12,567.    Elapsed: 0:44:41.\n",
      "  Batch 8,980  of  12,567.    Elapsed: 0:44:44.\n",
      "  Batch 8,990  of  12,567.    Elapsed: 0:44:47.\n",
      "  Batch 9,000  of  12,567.    Elapsed: 0:44:50.\n",
      "  Batch 9,010  of  12,567.    Elapsed: 0:44:53.\n",
      "  Batch 9,020  of  12,567.    Elapsed: 0:44:56.\n",
      "  Batch 9,030  of  12,567.    Elapsed: 0:44:59.\n",
      "  Batch 9,040  of  12,567.    Elapsed: 0:45:02.\n",
      "  Batch 9,050  of  12,567.    Elapsed: 0:45:05.\n",
      "  Batch 9,060  of  12,567.    Elapsed: 0:45:08.\n",
      "  Batch 9,070  of  12,567.    Elapsed: 0:45:11.\n",
      "  Batch 9,080  of  12,567.    Elapsed: 0:45:14.\n",
      "  Batch 9,090  of  12,567.    Elapsed: 0:45:17.\n",
      "  Batch 9,100  of  12,567.    Elapsed: 0:45:20.\n",
      "  Batch 9,110  of  12,567.    Elapsed: 0:45:23.\n",
      "  Batch 9,120  of  12,567.    Elapsed: 0:45:26.\n",
      "  Batch 9,130  of  12,567.    Elapsed: 0:45:29.\n",
      "  Batch 9,140  of  12,567.    Elapsed: 0:45:32.\n",
      "  Batch 9,150  of  12,567.    Elapsed: 0:45:35.\n",
      "  Batch 9,160  of  12,567.    Elapsed: 0:45:38.\n",
      "  Batch 9,170  of  12,567.    Elapsed: 0:45:41.\n",
      "  Batch 9,180  of  12,567.    Elapsed: 0:45:44.\n",
      "  Batch 9,190  of  12,567.    Elapsed: 0:45:48.\n",
      "  Batch 9,200  of  12,567.    Elapsed: 0:45:51.\n",
      "  Batch 9,210  of  12,567.    Elapsed: 0:45:54.\n",
      "  Batch 9,220  of  12,567.    Elapsed: 0:45:57.\n",
      "  Batch 9,230  of  12,567.    Elapsed: 0:46:00.\n",
      "  Batch 9,240  of  12,567.    Elapsed: 0:46:03.\n",
      "  Batch 9,250  of  12,567.    Elapsed: 0:46:06.\n",
      "  Batch 9,260  of  12,567.    Elapsed: 0:46:09.\n",
      "  Batch 9,270  of  12,567.    Elapsed: 0:46:12.\n",
      "  Batch 9,280  of  12,567.    Elapsed: 0:46:15.\n",
      "  Batch 9,290  of  12,567.    Elapsed: 0:46:18.\n",
      "  Batch 9,300  of  12,567.    Elapsed: 0:46:21.\n",
      "  Batch 9,310  of  12,567.    Elapsed: 0:46:24.\n",
      "  Batch 9,320  of  12,567.    Elapsed: 0:46:27.\n",
      "  Batch 9,330  of  12,567.    Elapsed: 0:46:30.\n",
      "  Batch 9,340  of  12,567.    Elapsed: 0:46:33.\n",
      "  Batch 9,350  of  12,567.    Elapsed: 0:46:36.\n",
      "  Batch 9,360  of  12,567.    Elapsed: 0:46:39.\n",
      "  Batch 9,370  of  12,567.    Elapsed: 0:46:42.\n",
      "  Batch 9,380  of  12,567.    Elapsed: 0:46:45.\n",
      "  Batch 9,390  of  12,567.    Elapsed: 0:46:48.\n",
      "  Batch 9,400  of  12,567.    Elapsed: 0:46:51.\n",
      "  Batch 9,410  of  12,567.    Elapsed: 0:46:54.\n",
      "  Batch 9,420  of  12,567.    Elapsed: 0:46:58.\n",
      "  Batch 9,430  of  12,567.    Elapsed: 0:47:01.\n",
      "  Batch 9,440  of  12,567.    Elapsed: 0:47:04.\n",
      "  Batch 9,450  of  12,567.    Elapsed: 0:47:07.\n",
      "  Batch 9,460  of  12,567.    Elapsed: 0:47:10.\n",
      "  Batch 9,470  of  12,567.    Elapsed: 0:47:13.\n",
      "  Batch 9,480  of  12,567.    Elapsed: 0:47:16.\n",
      "  Batch 9,490  of  12,567.    Elapsed: 0:47:19.\n",
      "  Batch 9,500  of  12,567.    Elapsed: 0:47:23.\n",
      "  Batch 9,510  of  12,567.    Elapsed: 0:47:26.\n",
      "  Batch 9,520  of  12,567.    Elapsed: 0:47:29.\n",
      "  Batch 9,530  of  12,567.    Elapsed: 0:47:32.\n",
      "  Batch 9,540  of  12,567.    Elapsed: 0:47:35.\n",
      "  Batch 9,550  of  12,567.    Elapsed: 0:47:38.\n",
      "  Batch 9,560  of  12,567.    Elapsed: 0:47:41.\n",
      "  Batch 9,570  of  12,567.    Elapsed: 0:47:44.\n",
      "  Batch 9,580  of  12,567.    Elapsed: 0:47:47.\n",
      "  Batch 9,590  of  12,567.    Elapsed: 0:47:50.\n",
      "  Batch 9,600  of  12,567.    Elapsed: 0:47:53.\n",
      "  Batch 9,610  of  12,567.    Elapsed: 0:47:56.\n",
      "  Batch 9,620  of  12,567.    Elapsed: 0:47:59.\n",
      "  Batch 9,630  of  12,567.    Elapsed: 0:48:02.\n",
      "  Batch 9,640  of  12,567.    Elapsed: 0:48:05.\n",
      "  Batch 9,650  of  12,567.    Elapsed: 0:48:08.\n",
      "  Batch 9,660  of  12,567.    Elapsed: 0:48:11.\n",
      "  Batch 9,670  of  12,567.    Elapsed: 0:48:14.\n",
      "  Batch 9,680  of  12,567.    Elapsed: 0:48:17.\n",
      "  Batch 9,690  of  12,567.    Elapsed: 0:48:20.\n",
      "  Batch 9,700  of  12,567.    Elapsed: 0:48:23.\n",
      "  Batch 9,710  of  12,567.    Elapsed: 0:48:27.\n",
      "  Batch 9,720  of  12,567.    Elapsed: 0:48:30.\n",
      "  Batch 9,730  of  12,567.    Elapsed: 0:48:33.\n",
      "  Batch 9,740  of  12,567.    Elapsed: 0:48:36.\n",
      "  Batch 9,750  of  12,567.    Elapsed: 0:48:39.\n",
      "  Batch 9,760  of  12,567.    Elapsed: 0:48:42.\n",
      "  Batch 9,770  of  12,567.    Elapsed: 0:48:45.\n",
      "  Batch 9,780  of  12,567.    Elapsed: 0:48:48.\n",
      "  Batch 9,790  of  12,567.    Elapsed: 0:48:51.\n",
      "  Batch 9,800  of  12,567.    Elapsed: 0:48:54.\n",
      "  Batch 9,810  of  12,567.    Elapsed: 0:48:57.\n",
      "  Batch 9,820  of  12,567.    Elapsed: 0:49:00.\n",
      "  Batch 9,830  of  12,567.    Elapsed: 0:49:03.\n",
      "  Batch 9,840  of  12,567.    Elapsed: 0:49:06.\n",
      "  Batch 9,850  of  12,567.    Elapsed: 0:49:09.\n",
      "  Batch 9,860  of  12,567.    Elapsed: 0:49:12.\n",
      "  Batch 9,870  of  12,567.    Elapsed: 0:49:15.\n",
      "  Batch 9,880  of  12,567.    Elapsed: 0:49:18.\n",
      "  Batch 9,890  of  12,567.    Elapsed: 0:49:21.\n",
      "  Batch 9,900  of  12,567.    Elapsed: 0:49:24.\n",
      "  Batch 9,910  of  12,567.    Elapsed: 0:49:27.\n",
      "  Batch 9,920  of  12,567.    Elapsed: 0:49:31.\n",
      "  Batch 9,930  of  12,567.    Elapsed: 0:49:34.\n",
      "  Batch 9,940  of  12,567.    Elapsed: 0:49:37.\n",
      "  Batch 9,950  of  12,567.    Elapsed: 0:49:40.\n",
      "  Batch 9,960  of  12,567.    Elapsed: 0:49:43.\n",
      "  Batch 9,970  of  12,567.    Elapsed: 0:49:46.\n",
      "  Batch 9,980  of  12,567.    Elapsed: 0:49:49.\n",
      "  Batch 9,990  of  12,567.    Elapsed: 0:49:52.\n",
      "  Batch 10,000  of  12,567.    Elapsed: 0:49:55.\n",
      "  Batch 10,010  of  12,567.    Elapsed: 0:49:58.\n",
      "  Batch 10,020  of  12,567.    Elapsed: 0:50:01.\n",
      "  Batch 10,030  of  12,567.    Elapsed: 0:50:05.\n",
      "  Batch 10,040  of  12,567.    Elapsed: 0:50:08.\n",
      "  Batch 10,050  of  12,567.    Elapsed: 0:50:11.\n",
      "  Batch 10,060  of  12,567.    Elapsed: 0:50:14.\n",
      "  Batch 10,070  of  12,567.    Elapsed: 0:50:17.\n",
      "  Batch 10,080  of  12,567.    Elapsed: 0:50:20.\n",
      "  Batch 10,090  of  12,567.    Elapsed: 0:50:23.\n",
      "  Batch 10,100  of  12,567.    Elapsed: 0:50:27.\n",
      "  Batch 10,110  of  12,567.    Elapsed: 0:50:30.\n",
      "  Batch 10,120  of  12,567.    Elapsed: 0:50:33.\n",
      "  Batch 10,130  of  12,567.    Elapsed: 0:50:36.\n",
      "  Batch 10,140  of  12,567.    Elapsed: 0:50:39.\n",
      "  Batch 10,150  of  12,567.    Elapsed: 0:50:42.\n",
      "  Batch 10,160  of  12,567.    Elapsed: 0:50:45.\n",
      "  Batch 10,170  of  12,567.    Elapsed: 0:50:49.\n",
      "  Batch 10,180  of  12,567.    Elapsed: 0:50:52.\n",
      "  Batch 10,190  of  12,567.    Elapsed: 0:50:55.\n",
      "  Batch 10,200  of  12,567.    Elapsed: 0:50:59.\n",
      "  Batch 10,210  of  12,567.    Elapsed: 0:51:02.\n",
      "  Batch 10,220  of  12,567.    Elapsed: 0:51:05.\n",
      "  Batch 10,230  of  12,567.    Elapsed: 0:51:08.\n",
      "  Batch 10,240  of  12,567.    Elapsed: 0:51:11.\n",
      "  Batch 10,250  of  12,567.    Elapsed: 0:51:14.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10,260  of  12,567.    Elapsed: 0:51:18.\n",
      "  Batch 10,270  of  12,567.    Elapsed: 0:51:21.\n",
      "  Batch 10,280  of  12,567.    Elapsed: 0:51:24.\n",
      "  Batch 10,290  of  12,567.    Elapsed: 0:51:27.\n",
      "  Batch 10,300  of  12,567.    Elapsed: 0:51:30.\n",
      "  Batch 10,310  of  12,567.    Elapsed: 0:51:33.\n",
      "  Batch 10,320  of  12,567.    Elapsed: 0:51:37.\n",
      "  Batch 10,330  of  12,567.    Elapsed: 0:51:40.\n",
      "  Batch 10,340  of  12,567.    Elapsed: 0:51:43.\n",
      "  Batch 10,350  of  12,567.    Elapsed: 0:51:46.\n",
      "  Batch 10,360  of  12,567.    Elapsed: 0:51:49.\n",
      "  Batch 10,370  of  12,567.    Elapsed: 0:51:52.\n",
      "  Batch 10,380  of  12,567.    Elapsed: 0:51:55.\n",
      "  Batch 10,390  of  12,567.    Elapsed: 0:51:58.\n",
      "  Batch 10,400  of  12,567.    Elapsed: 0:52:01.\n",
      "  Batch 10,410  of  12,567.    Elapsed: 0:52:04.\n",
      "  Batch 10,420  of  12,567.    Elapsed: 0:52:07.\n",
      "  Batch 10,430  of  12,567.    Elapsed: 0:52:10.\n",
      "  Batch 10,440  of  12,567.    Elapsed: 0:52:13.\n",
      "  Batch 10,450  of  12,567.    Elapsed: 0:52:16.\n",
      "  Batch 10,460  of  12,567.    Elapsed: 0:52:19.\n",
      "  Batch 10,470  of  12,567.    Elapsed: 0:52:22.\n",
      "  Batch 10,480  of  12,567.    Elapsed: 0:52:25.\n",
      "  Batch 10,490  of  12,567.    Elapsed: 0:52:28.\n",
      "  Batch 10,500  of  12,567.    Elapsed: 0:52:31.\n",
      "  Batch 10,510  of  12,567.    Elapsed: 0:52:34.\n",
      "  Batch 10,520  of  12,567.    Elapsed: 0:52:37.\n",
      "  Batch 10,530  of  12,567.    Elapsed: 0:52:40.\n",
      "  Batch 10,540  of  12,567.    Elapsed: 0:52:43.\n",
      "  Batch 10,550  of  12,567.    Elapsed: 0:52:46.\n",
      "  Batch 10,560  of  12,567.    Elapsed: 0:52:49.\n",
      "  Batch 10,570  of  12,567.    Elapsed: 0:52:52.\n",
      "  Batch 10,580  of  12,567.    Elapsed: 0:52:55.\n",
      "  Batch 10,590  of  12,567.    Elapsed: 0:52:58.\n",
      "  Batch 10,600  of  12,567.    Elapsed: 0:53:01.\n",
      "  Batch 10,610  of  12,567.    Elapsed: 0:53:04.\n",
      "  Batch 10,620  of  12,567.    Elapsed: 0:53:07.\n",
      "  Batch 10,630  of  12,567.    Elapsed: 0:53:10.\n",
      "  Batch 10,640  of  12,567.    Elapsed: 0:53:13.\n",
      "  Batch 10,650  of  12,567.    Elapsed: 0:53:16.\n",
      "  Batch 10,660  of  12,567.    Elapsed: 0:53:19.\n",
      "  Batch 10,670  of  12,567.    Elapsed: 0:53:22.\n",
      "  Batch 10,680  of  12,567.    Elapsed: 0:53:25.\n",
      "  Batch 10,690  of  12,567.    Elapsed: 0:53:28.\n",
      "  Batch 10,700  of  12,567.    Elapsed: 0:53:31.\n",
      "  Batch 10,710  of  12,567.    Elapsed: 0:53:34.\n",
      "  Batch 10,720  of  12,567.    Elapsed: 0:53:37.\n",
      "  Batch 10,730  of  12,567.    Elapsed: 0:53:40.\n",
      "  Batch 10,740  of  12,567.    Elapsed: 0:53:43.\n",
      "  Batch 10,750  of  12,567.    Elapsed: 0:53:46.\n",
      "  Batch 10,760  of  12,567.    Elapsed: 0:53:49.\n",
      "  Batch 10,770  of  12,567.    Elapsed: 0:53:52.\n",
      "  Batch 10,780  of  12,567.    Elapsed: 0:53:55.\n",
      "  Batch 10,790  of  12,567.    Elapsed: 0:53:58.\n",
      "  Batch 10,800  of  12,567.    Elapsed: 0:54:01.\n",
      "  Batch 10,810  of  12,567.    Elapsed: 0:54:04.\n",
      "  Batch 10,820  of  12,567.    Elapsed: 0:54:07.\n",
      "  Batch 10,830  of  12,567.    Elapsed: 0:54:10.\n",
      "  Batch 10,840  of  12,567.    Elapsed: 0:54:13.\n",
      "  Batch 10,850  of  12,567.    Elapsed: 0:54:16.\n",
      "  Batch 10,860  of  12,567.    Elapsed: 0:54:19.\n",
      "  Batch 10,870  of  12,567.    Elapsed: 0:54:22.\n",
      "  Batch 10,880  of  12,567.    Elapsed: 0:54:25.\n",
      "  Batch 10,890  of  12,567.    Elapsed: 0:54:28.\n",
      "  Batch 10,900  of  12,567.    Elapsed: 0:54:31.\n",
      "  Batch 10,910  of  12,567.    Elapsed: 0:54:34.\n",
      "  Batch 10,920  of  12,567.    Elapsed: 0:54:37.\n",
      "  Batch 10,930  of  12,567.    Elapsed: 0:54:40.\n",
      "  Batch 10,940  of  12,567.    Elapsed: 0:54:43.\n",
      "  Batch 10,950  of  12,567.    Elapsed: 0:54:46.\n",
      "  Batch 10,960  of  12,567.    Elapsed: 0:54:49.\n",
      "  Batch 10,970  of  12,567.    Elapsed: 0:54:52.\n",
      "  Batch 10,980  of  12,567.    Elapsed: 0:54:55.\n",
      "  Batch 10,990  of  12,567.    Elapsed: 0:54:58.\n",
      "  Batch 11,000  of  12,567.    Elapsed: 0:55:01.\n",
      "  Batch 11,010  of  12,567.    Elapsed: 0:55:04.\n",
      "  Batch 11,020  of  12,567.    Elapsed: 0:55:07.\n",
      "  Batch 11,030  of  12,567.    Elapsed: 0:55:10.\n",
      "  Batch 11,040  of  12,567.    Elapsed: 0:55:13.\n",
      "  Batch 11,050  of  12,567.    Elapsed: 0:55:16.\n",
      "  Batch 11,060  of  12,567.    Elapsed: 0:55:19.\n",
      "  Batch 11,070  of  12,567.    Elapsed: 0:55:22.\n",
      "  Batch 11,080  of  12,567.    Elapsed: 0:55:25.\n",
      "  Batch 11,090  of  12,567.    Elapsed: 0:55:28.\n",
      "  Batch 11,100  of  12,567.    Elapsed: 0:55:31.\n",
      "  Batch 11,110  of  12,567.    Elapsed: 0:55:34.\n",
      "  Batch 11,120  of  12,567.    Elapsed: 0:55:37.\n",
      "  Batch 11,130  of  12,567.    Elapsed: 0:55:40.\n",
      "  Batch 11,140  of  12,567.    Elapsed: 0:55:43.\n",
      "  Batch 11,150  of  12,567.    Elapsed: 0:55:46.\n",
      "  Batch 11,160  of  12,567.    Elapsed: 0:55:49.\n",
      "  Batch 11,170  of  12,567.    Elapsed: 0:55:52.\n",
      "  Batch 11,180  of  12,567.    Elapsed: 0:55:55.\n",
      "  Batch 11,190  of  12,567.    Elapsed: 0:55:58.\n",
      "  Batch 11,200  of  12,567.    Elapsed: 0:56:01.\n",
      "  Batch 11,210  of  12,567.    Elapsed: 0:56:04.\n",
      "  Batch 11,220  of  12,567.    Elapsed: 0:56:07.\n",
      "  Batch 11,230  of  12,567.    Elapsed: 0:56:10.\n",
      "  Batch 11,240  of  12,567.    Elapsed: 0:56:13.\n",
      "  Batch 11,250  of  12,567.    Elapsed: 0:56:16.\n",
      "  Batch 11,260  of  12,567.    Elapsed: 0:56:19.\n",
      "  Batch 11,270  of  12,567.    Elapsed: 0:56:22.\n",
      "  Batch 11,280  of  12,567.    Elapsed: 0:56:25.\n",
      "  Batch 11,290  of  12,567.    Elapsed: 0:56:28.\n",
      "  Batch 11,300  of  12,567.    Elapsed: 0:56:31.\n",
      "  Batch 11,310  of  12,567.    Elapsed: 0:56:34.\n",
      "  Batch 11,320  of  12,567.    Elapsed: 0:56:37.\n",
      "  Batch 11,330  of  12,567.    Elapsed: 0:56:40.\n",
      "  Batch 11,340  of  12,567.    Elapsed: 0:56:43.\n",
      "  Batch 11,350  of  12,567.    Elapsed: 0:56:46.\n",
      "  Batch 11,360  of  12,567.    Elapsed: 0:56:49.\n",
      "  Batch 11,370  of  12,567.    Elapsed: 0:56:52.\n",
      "  Batch 11,380  of  12,567.    Elapsed: 0:56:55.\n",
      "  Batch 11,390  of  12,567.    Elapsed: 0:56:58.\n",
      "  Batch 11,400  of  12,567.    Elapsed: 0:57:01.\n",
      "  Batch 11,410  of  12,567.    Elapsed: 0:57:04.\n",
      "  Batch 11,420  of  12,567.    Elapsed: 0:57:07.\n",
      "  Batch 11,430  of  12,567.    Elapsed: 0:57:10.\n",
      "  Batch 11,440  of  12,567.    Elapsed: 0:57:13.\n",
      "  Batch 11,450  of  12,567.    Elapsed: 0:57:17.\n",
      "  Batch 11,460  of  12,567.    Elapsed: 0:57:20.\n",
      "  Batch 11,470  of  12,567.    Elapsed: 0:57:23.\n",
      "  Batch 11,480  of  12,567.    Elapsed: 0:57:26.\n",
      "  Batch 11,490  of  12,567.    Elapsed: 0:57:29.\n",
      "  Batch 11,500  of  12,567.    Elapsed: 0:57:32.\n",
      "  Batch 11,510  of  12,567.    Elapsed: 0:57:35.\n",
      "  Batch 11,520  of  12,567.    Elapsed: 0:57:38.\n",
      "  Batch 11,530  of  12,567.    Elapsed: 0:57:41.\n",
      "  Batch 11,540  of  12,567.    Elapsed: 0:57:44.\n",
      "  Batch 11,550  of  12,567.    Elapsed: 0:57:47.\n",
      "  Batch 11,560  of  12,567.    Elapsed: 0:57:49.\n",
      "  Batch 11,570  of  12,567.    Elapsed: 0:57:52.\n",
      "  Batch 11,580  of  12,567.    Elapsed: 0:57:55.\n",
      "  Batch 11,590  of  12,567.    Elapsed: 0:57:58.\n",
      "  Batch 11,600  of  12,567.    Elapsed: 0:58:01.\n",
      "  Batch 11,610  of  12,567.    Elapsed: 0:58:04.\n",
      "  Batch 11,620  of  12,567.    Elapsed: 0:58:07.\n",
      "  Batch 11,630  of  12,567.    Elapsed: 0:58:10.\n",
      "  Batch 11,640  of  12,567.    Elapsed: 0:58:13.\n",
      "  Batch 11,650  of  12,567.    Elapsed: 0:58:16.\n",
      "  Batch 11,660  of  12,567.    Elapsed: 0:58:19.\n",
      "  Batch 11,670  of  12,567.    Elapsed: 0:58:22.\n",
      "  Batch 11,680  of  12,567.    Elapsed: 0:58:25.\n",
      "  Batch 11,690  of  12,567.    Elapsed: 0:58:28.\n",
      "  Batch 11,700  of  12,567.    Elapsed: 0:58:31.\n",
      "  Batch 11,710  of  12,567.    Elapsed: 0:58:34.\n",
      "  Batch 11,720  of  12,567.    Elapsed: 0:58:37.\n",
      "  Batch 11,730  of  12,567.    Elapsed: 0:58:40.\n",
      "  Batch 11,740  of  12,567.    Elapsed: 0:58:43.\n",
      "  Batch 11,750  of  12,567.    Elapsed: 0:58:46.\n",
      "  Batch 11,760  of  12,567.    Elapsed: 0:58:49.\n",
      "  Batch 11,770  of  12,567.    Elapsed: 0:58:52.\n",
      "  Batch 11,780  of  12,567.    Elapsed: 0:58:55.\n",
      "  Batch 11,790  of  12,567.    Elapsed: 0:58:58.\n",
      "  Batch 11,800  of  12,567.    Elapsed: 0:59:01.\n",
      "  Batch 11,810  of  12,567.    Elapsed: 0:59:04.\n",
      "  Batch 11,820  of  12,567.    Elapsed: 0:59:07.\n",
      "  Batch 11,830  of  12,567.    Elapsed: 0:59:10.\n",
      "  Batch 11,840  of  12,567.    Elapsed: 0:59:13.\n",
      "  Batch 11,850  of  12,567.    Elapsed: 0:59:16.\n",
      "  Batch 11,860  of  12,567.    Elapsed: 0:59:19.\n",
      "  Batch 11,870  of  12,567.    Elapsed: 0:59:22.\n",
      "  Batch 11,880  of  12,567.    Elapsed: 0:59:25.\n",
      "  Batch 11,890  of  12,567.    Elapsed: 0:59:28.\n",
      "  Batch 11,900  of  12,567.    Elapsed: 0:59:31.\n",
      "  Batch 11,910  of  12,567.    Elapsed: 0:59:34.\n",
      "  Batch 11,920  of  12,567.    Elapsed: 0:59:37.\n",
      "  Batch 11,930  of  12,567.    Elapsed: 0:59:40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 11,940  of  12,567.    Elapsed: 0:59:43.\n",
      "  Batch 11,950  of  12,567.    Elapsed: 0:59:46.\n",
      "  Batch 11,960  of  12,567.    Elapsed: 0:59:49.\n",
      "  Batch 11,970  of  12,567.    Elapsed: 0:59:52.\n",
      "  Batch 11,980  of  12,567.    Elapsed: 0:59:55.\n",
      "  Batch 11,990  of  12,567.    Elapsed: 0:59:58.\n",
      "  Batch 12,000  of  12,567.    Elapsed: 1:00:01.\n",
      "  Batch 12,010  of  12,567.    Elapsed: 1:00:04.\n",
      "  Batch 12,020  of  12,567.    Elapsed: 1:00:07.\n",
      "  Batch 12,030  of  12,567.    Elapsed: 1:00:10.\n",
      "  Batch 12,040  of  12,567.    Elapsed: 1:00:13.\n",
      "  Batch 12,050  of  12,567.    Elapsed: 1:00:16.\n",
      "  Batch 12,060  of  12,567.    Elapsed: 1:00:19.\n",
      "  Batch 12,070  of  12,567.    Elapsed: 1:00:22.\n",
      "  Batch 12,080  of  12,567.    Elapsed: 1:00:24.\n",
      "  Batch 12,090  of  12,567.    Elapsed: 1:00:27.\n",
      "  Batch 12,100  of  12,567.    Elapsed: 1:00:30.\n",
      "  Batch 12,110  of  12,567.    Elapsed: 1:00:33.\n",
      "  Batch 12,120  of  12,567.    Elapsed: 1:00:36.\n",
      "  Batch 12,130  of  12,567.    Elapsed: 1:00:39.\n",
      "  Batch 12,140  of  12,567.    Elapsed: 1:00:42.\n",
      "  Batch 12,150  of  12,567.    Elapsed: 1:00:45.\n",
      "  Batch 12,160  of  12,567.    Elapsed: 1:00:48.\n",
      "  Batch 12,170  of  12,567.    Elapsed: 1:00:51.\n",
      "  Batch 12,180  of  12,567.    Elapsed: 1:00:54.\n",
      "  Batch 12,190  of  12,567.    Elapsed: 1:00:57.\n",
      "  Batch 12,200  of  12,567.    Elapsed: 1:01:00.\n",
      "  Batch 12,210  of  12,567.    Elapsed: 1:01:03.\n",
      "  Batch 12,220  of  12,567.    Elapsed: 1:01:06.\n",
      "  Batch 12,230  of  12,567.    Elapsed: 1:01:09.\n",
      "  Batch 12,240  of  12,567.    Elapsed: 1:01:12.\n",
      "  Batch 12,250  of  12,567.    Elapsed: 1:01:15.\n",
      "  Batch 12,260  of  12,567.    Elapsed: 1:01:18.\n",
      "  Batch 12,270  of  12,567.    Elapsed: 1:01:21.\n",
      "  Batch 12,280  of  12,567.    Elapsed: 1:01:24.\n",
      "  Batch 12,290  of  12,567.    Elapsed: 1:01:27.\n",
      "  Batch 12,300  of  12,567.    Elapsed: 1:01:30.\n",
      "  Batch 12,310  of  12,567.    Elapsed: 1:01:33.\n",
      "  Batch 12,320  of  12,567.    Elapsed: 1:01:36.\n",
      "  Batch 12,330  of  12,567.    Elapsed: 1:01:39.\n",
      "  Batch 12,340  of  12,567.    Elapsed: 1:01:42.\n",
      "  Batch 12,350  of  12,567.    Elapsed: 1:01:45.\n",
      "  Batch 12,360  of  12,567.    Elapsed: 1:01:48.\n",
      "  Batch 12,370  of  12,567.    Elapsed: 1:01:51.\n",
      "  Batch 12,380  of  12,567.    Elapsed: 1:01:54.\n",
      "  Batch 12,390  of  12,567.    Elapsed: 1:01:57.\n",
      "  Batch 12,400  of  12,567.    Elapsed: 1:02:00.\n",
      "  Batch 12,410  of  12,567.    Elapsed: 1:02:03.\n",
      "  Batch 12,420  of  12,567.    Elapsed: 1:02:06.\n",
      "  Batch 12,430  of  12,567.    Elapsed: 1:02:09.\n",
      "  Batch 12,440  of  12,567.    Elapsed: 1:02:12.\n",
      "  Batch 12,450  of  12,567.    Elapsed: 1:02:15.\n",
      "  Batch 12,460  of  12,567.    Elapsed: 1:02:18.\n",
      "  Batch 12,470  of  12,567.    Elapsed: 1:02:21.\n",
      "  Batch 12,480  of  12,567.    Elapsed: 1:02:24.\n",
      "  Batch 12,490  of  12,567.    Elapsed: 1:02:27.\n",
      "  Batch 12,500  of  12,567.    Elapsed: 1:02:30.\n",
      "  Batch 12,510  of  12,567.    Elapsed: 1:02:33.\n",
      "  Batch 12,520  of  12,567.    Elapsed: 1:02:36.\n",
      "  Batch 12,530  of  12,567.    Elapsed: 1:02:39.\n",
      "  Batch 12,540  of  12,567.    Elapsed: 1:02:42.\n",
      "  Batch 12,550  of  12,567.    Elapsed: 1:02:45.\n",
      "  Batch 12,560  of  12,567.    Elapsed: 1:02:48.\n",
      "\n",
      "  Average training loss generetor: 0.706\n",
      "  Average training loss discriminator: 1.171\n",
      "  Training epcoh took: 1:02:50\n",
      "Saving the models...............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\komol\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.540\n",
      "  Test Loss: 0.690\n",
      "  Test took: 0:00:11\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "#models parameters\n",
    "transformer_vars = [i for i in transformer.parameters()]\n",
    "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
    "g_vars = [v for v in generator.parameters()]\n",
    "\n",
    "#optimizer\n",
    "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
    "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
    "\n",
    "#scheduler\n",
    "if apply_scheduler:\n",
    "  num_train_examples = len(train_examples)\n",
    "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
    "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
    "\n",
    "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
    "                                           num_warmup_steps = num_warmup_steps)\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, num_train_epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    tr_g_loss = 0\n",
    "    tr_d_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    transformer.train() \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every print_each_n_step batches.\n",
    "        if step % print_each_n_step == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_label_mask = batch[3].to(device)\n",
    "        \n",
    "        # Encode real data in the Transformer\n",
    "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "        hidden_states = model_outputs[-1]\n",
    "        \n",
    "        # Generate fake data that should have the same distribution of the ones\n",
    "        # encoded by the transformer. \n",
    "        # First noisy input are used in input to the Generator\n",
    "        noise = torch.zeros(b_input_ids.shape[0],noise_size, device=device).uniform_(0, 1)\n",
    "        # Gnerate Fake data\n",
    "        gen_rep = generator(noise)\n",
    "\n",
    "        # Generate the output of the Discriminator for real and fake data.\n",
    "        # First, we put together the output of the tranformer and the generator\n",
    "        disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
    "        # Then, we select the output of the disciminator\n",
    "        features, logits, probs = discriminator(disciminator_input)\n",
    "\n",
    "        # Finally, we separate the discriminator's output for the real and fake\n",
    "        # data\n",
    "        features_list = torch.split(features, batch_size)\n",
    "        D_real_features = features_list[0]\n",
    "        D_fake_features = features_list[1]\n",
    "        \n",
    "        logits_list = torch.split(logits, batch_size)\n",
    "        D_real_logits = logits_list[0]\n",
    "        D_fake_logits = logits_list[1]\n",
    "        \n",
    "        probs_list = torch.split(probs, batch_size)\n",
    "        D_real_probs = probs_list[0]\n",
    "        D_fake_probs = probs_list[1]\n",
    "\n",
    "        #---------------------------------\n",
    "        #  LOSS evaluation\n",
    "        #---------------------------------\n",
    "        # Generator's LOSS estimation\n",
    "        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
    "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
    "        g_loss = g_loss_d + g_feat_reg\n",
    "  \n",
    "        # Disciminator's LOSS estimation\n",
    "        logits = D_real_logits[:,0:-1]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        # The discriminator provides an output for labeled and unlabeled real data\n",
    "        # so the loss evaluated for unlabeled data is ignored (masked)\n",
    "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
    "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
    "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
    "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
    "\n",
    "        # It may be the case that a batch does not contain labeled examples, \n",
    "        # so the \"supervised loss\" in this case is not evaluated\n",
    "        if labeled_example_count == 0:\n",
    "          D_L_Supervised = 0\n",
    "        else:\n",
    "          D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
    "                 \n",
    "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
    "        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
    "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
    "\n",
    "        #---------------------------------\n",
    "        #  OPTIMIZATION\n",
    "        #---------------------------------\n",
    "        # Avoid gradient accumulation\n",
    "        gen_optimizer.zero_grad()\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "        # Calculate weigth updates\n",
    "        # retain_graph=True is required since the underlying graph will be deleted after backward\n",
    "        g_loss.backward(retain_graph=True)\n",
    "        d_loss.backward() \n",
    "        \n",
    "        # Apply modifications\n",
    "        gen_optimizer.step()\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        # A detail log of the individual losses\n",
    "        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
    "        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
    "        #             g_loss_d, g_feat_reg))\n",
    "\n",
    "        # Save the losses to print them later\n",
    "        tr_g_loss += g_loss.item()\n",
    "        tr_d_loss += d_loss.item()\n",
    "\n",
    "        # Update the learning rate with the scheduler\n",
    "        if apply_scheduler:\n",
    "          scheduler_d.step()\n",
    "          scheduler_g.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
    "    avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
    "    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "    print(\"Saving the models...............................\")\n",
    "    # Saving the model\n",
    "    torch.save(transformer, 'transformer')\n",
    "    torch.save(discriminator, 'discriminator')\n",
    "\n",
    "        \n",
    "    # ========================================\n",
    "    #     TEST ON THE EVALUATION DATASET\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our test set.\n",
    "    print(\"\")\n",
    "    print(\"Running Test...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    transformer.eval() #maybe redundant\n",
    "    discriminator.eval()\n",
    "    generator.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_test_accuracy = 0\n",
    "   \n",
    "    total_test_loss = 0\n",
    "    nb_test_steps = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels_ids = []\n",
    "\n",
    "    #loss\n",
    "    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "            hidden_states = model_outputs[-1]\n",
    "            _, logits, probs = discriminator(hidden_states)\n",
    "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "            filtered_logits = logits[:,0:-1]\n",
    "            # Accumulate the test loss.\n",
    "            total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "            \n",
    "        # Accumulate the predictions and the input labels\n",
    "        _, preds = torch.max(filtered_logits, 1)\n",
    "        all_preds += preds.detach().cpu()\n",
    "        all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    all_preds = torch.stack(all_preds).numpy()\n",
    "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    avg_test_loss = avg_test_loss.item()\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    test_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "    print(\"  Test took: {:}\".format(test_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss generator': avg_train_loss_g,\n",
    "            'Training Loss discriminator': avg_train_loss_d,\n",
    "            'Valid. Loss': avg_test_loss,\n",
    "            'Valid. Accur.': test_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Test Time': test_time\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Test...\n",
      "  Accuracy: 0.540\n",
      "  Test Loss: 0.690\n",
      "  Test took: 0:00:11\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"Running Test...\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Put the model in evaluation mode--the dropout layers behave differently\n",
    "# during evaluation.\n",
    "transformer.eval() #maybe redundant\n",
    "discriminator.eval()\n",
    "\n",
    "# Tracking variables \n",
    "total_test_accuracy = 0\n",
    "\n",
    "total_test_loss = 0\n",
    "nb_test_steps = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels_ids = []\n",
    "\n",
    "#loss\n",
    "nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in test_dataloader:\n",
    "    \n",
    "    # Unpack this training batch from our dataloader. \n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    \n",
    "    # Tell pytorch not to bother with constructing the compute graph during\n",
    "    # the forward pass, since this is only needed for backprop (training).\n",
    "    with torch.no_grad():        \n",
    "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
    "        hidden_states = model_outputs[-1]\n",
    "        _, logits, probs = discriminator(hidden_states)\n",
    "        ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
    "        filtered_logits = logits[:,0:-1]\n",
    "        # Accumulate the test loss.\n",
    "        total_test_loss += nll_loss(filtered_logits, b_labels)\n",
    "        \n",
    "    # Accumulate the predictions and the input labels\n",
    "    _, preds = torch.max(filtered_logits, 1)\n",
    "    all_preds += preds.detach().cpu()\n",
    "    all_labels_ids += b_labels.detach().cpu()\n",
    "\n",
    "# Report the final accuracy for this validation run.\n",
    "all_preds = torch.stack(all_preds).numpy()\n",
    "all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
    "test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
    "print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
    "\n",
    "# Calculate the average loss over all of the batches.\n",
    "avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "avg_test_loss = avg_test_loss.item()\n",
    "\n",
    "# Measure how long the validation run took.\n",
    "test_time = format_time(time.time() - t0)\n",
    "  \n",
    "print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
    "print(\"  Test took: {:}\".format(test_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
